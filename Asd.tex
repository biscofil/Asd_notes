\documentclass{article}

\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript

% Abs
\usepackage{mathtools,etoolbox}
\DeclarePairedDelimiterX{\abs}[1]{\lvert}{\rvert}{\ifblank{#1}{{}\cdot{}}{#1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Tables 
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% Trees
\usepackage{graphviz}
\usepackage{tikz}
\usetikzlibrary{arrows}
\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,font=\sffamily},
  arn_n/.style = {treenode, circle, white, font=\sffamily\bfseries, draw=black,
    fill=black, text width=1.5em},% arbre rouge noir, noeud noir
  arn_r/.style = {treenode, circle, red, draw=red, 
    text width=1.5em, very thick},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=0.5em, minimum height=0.5em}% arbre rouge noir, nil
}

%
\usepackage{amssymb}

% Code blocks
\usepackage{listings}
\usepackage{color} 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinelanguage{pseudocode}{
	sensitive=false,
	keywords={}
	otherkeywords={<,<=,>,>=,==,=, <-,->, !=}, %operators
	keywords=[2]{foreach, while, if, else, then, do, return, repeat, until, and, or, in},
	keywords=[3]{true, false, nil, null},
	keywordstyle=\color{black},
	keywordstyle=[2]\color{blue},
	keywordstyle=[3]\color{red},
	comment=[l]{//},
	commentstyle=\color{grey},
	stringstyle=\color{red}
	numberstyle=\scriptsize
}

\lstdefinestyle{pseudocodestyle}{
	language=pseudocode,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    mathescape=true
}
\lstset{style=pseudocodestyle}

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Appunti di Algoritmi e strutture dati}
\author{Filippo Bisconcin}
\date{2018}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\tableofcontents

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{Algoritmi e loro complessità: i numeri di Fibonacci}

{~{[}DFI{]} cap. 1}

\begin{equation}
f(n) = 
\begin{cases}
1 & \mbox{se } n=1,n=2 \\ 
(n-1)+f(n-2) & \mbox{se } n\geq3 
\end{cases}
\end{equation}

\subsection{Formula di Binet}

\begin{equation}
\forall n \in N, F_n = \frac{1}{\sqrt{5}}(\Phi^n-\overline{\Phi}^n)
\end{equation}

Dove con $\Phi$ si indica la sezione aurea $\frac{1+\sqrt{5}}{2} \simeq -0,618$ e con $\overline{\Phi}$ si indica $\frac{1-\sqrt{5}}{2} \simeq -0,618$

\subsubsection{Dimostrazione}

{Dimostriamo per Induzione la formula di Binet:}

{Base:}

{$n = 1 \rightarrow F_n = \frac{1}{\sqrt{5}}(\Phi^1-\overline{\Phi}^1)$

{Sostistuisco con le definizioni di $\Phi$ e semplifico}

{$F_n = 1 = F_1$}

{$n = 2 \rightarrow F_n =\frac{1}{\sqrt{5}}(\Phi^2-\overline{\Phi}^2)$

{Sostistuisco con le definizioni di $\Phi$ e semplifico}

{$F_n = 1 = F_2$}

{Passo induttivo $n \geq 3$ :}

$F_n = F_{n-1} + F_{n-2}$

$F_{n-1}=\frac{1}{\sqrt{5}}(\Phi^{n-1}-\overline{\Phi}^{n-1})$ e 
$F_{n-2}=\frac{1}{\sqrt{5}}(\Phi^{n-2}-\overline{\Phi}^{n-2})$

{assomiglia alla forma $F_n=\frac{1}{\sqrt{5}}(\Phi^n-\overline{\Phi}^n)$}

{ci chiediamo se $\Phi^n=\Phi^{n-1}+\Phi^{n-2}$ e se $\overline{\Phi}^n=\overline{\Phi}^{n-1}+\overline{\Phi}^{n-2}$

{divido da entrambe le parti per $\Phi^{n-2}$ e $\overline{\Phi}^{n-2}$}

\paragraph{Definizione 1}

{Utilizzeremo la notazione $T_n$ per indicare la velocità/complessità di una particolare funzione (numero di righe di codice eseguite) }

\lstinputlisting{code/fib1.txt}

$T(Fib1)_n = 1\,\forall n$

{Problema: con l'aumentare di $n$, la funzione $Fib1(n)$ diventa sempre più imprecisa}

$Fib1_3=1,9999.. \simeq 2$ \\
$Fib1_{16}=986,699.. \simeq 987$ \\
$Fib1_{18}=2583,1.. \simeq 2584$

\paragraph{Definizione 2}

\lstinputlisting{code/fib2.txt}

\begin{equation}
T(Fib2)_n = 
\begin{cases}
1 & \mbox{se } n=1,n=2 \\ 
2 + T_{n-1} + T_{n-2} & \mbox{se } \forall n \geq 3 
\end{cases}
\end{equation}

{$T(Fib2)_n$ è una formula ricorsiva o ricorrenza}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{n}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
$T_n$\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{1}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{1}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{2}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{1}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{3}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{2 + 1 + 1 = 4}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{4}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{2 + 4 + 1 = 7}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{5}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{2 + 7 + 4 = 13}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

{}

{Risolviamo la ricorrenza per determinare la complessità / bontà della
funzione esaminata.}

\begin{itemize}
\tightlist
\item
  {andamento esponenziale rispetto ad $n$: funzione NON efficiente}
\item
  {andamento logaritmico / proporzionale rispetto ad $n$: funzione
  efficiente}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{h.x4ciu865ga1f}{\subsubsection{\texorpdfstring{{Strumento
\#1 - albero di
ricorsione}}{Strumento \#1 - albero di ricorsione}}\label{h.x4ciu865ga1f}}

{Notiamo che $T(Fib2)_5=(1*5)+(2*4)=13$, dove $(1 * 5)$ è la complessità delle foglie (5 foglie con peso 1) e $(2 * 4)$ è la complessità dei nodi interni (4 nodi con peso 2)}

{\includegraphics{images/image523.png}}

\paragraph{Proprietà 1}

{Se $T_n$ rappresenta l'albero di ricorsione relativo a $Fib2_n$, allora il numero di foglie di $T_n$ è uguale a $F_n$(con $F_n$ indichiamo l'n-esimo numero della successione di Fibonacci)}

{Dimostrazione per induzione}

{Base:}

{se $n = 1$, il numero di foglie di $T_2$, $F_1=1$}

{se $n = 2$, il numero di foglie di $T_2$, $F_2=1$}

{Passo induttivo:}

{\includegraphics{images/image528.png}}

\paragraph{Proprietà 2}

{Sia $T$ un albero binario in cui ogni nodo interno ha esattamente 2 figli}

{Allora $i_T=f_T-1$, dove con $i_T$ indichiamo il numero di nodi interni dell'albero $T$ e con $f_T$ indichiamo il numero di foglie dell'albero $T$}

\paragraph{Dimostrazione per induzione}

{Dimostrazione su $n$, dove $n$ è il numero di nodi di $T$}

{Base $n=1$ : OVVIO}

{Passo induttivo: $n \geq 2$}

{\includegraphics{images/image525.png}}

{\includegraphics{images/image536.png}}

{L'albero risultante lo chiamiamo $\overline{T}$:}

{\includegraphics{images/image522.png}}

{Notiamo che }

$t_{\overline{T}} = f_{\overline{T}}-1${*}

$f_{\overline{T}} = f_{T}-1${**}

$i_{\overline{T}} = i_{T}-1$

{→ (inverto) $i_T=i_{\overline{T}}+1$}

{→ (sostituisco*) $i_T=f_{\overline{T}}+1-1$}

{→ (semplifico) $i_T=f_{\overline{T}}$}

{→ (sostituisco**) $i_T=f_T-1$}

{Studio complessità di $Fib2_n$ con l'utilizzo del metodo dell'albero di ricorsione}

$i_{T_n}=F_n-1$

$f_{T_n}=F_n$

$T_n = 2 * i_{T_n}+1*f_{T_n}$

{→ $2 * F_{n-1} + F_n$}

{→ $3 * F_n - 2$}

\paragraph{Proprietà 3}

\begin{equation}
\forall n \geq 6 \rightarrow F_n \geq 2^{\frac{n}{2}}
\end{equation}

{Dimostrazione per induzione}

{Base $n=6$ :}

$F_6=8$	\\
$2^{\frac{6}{3}}=2^3=8$

{Passo induttivo $n \geq 7$ :}

$F(n) \geq 2^{\frac{n-1}{2}} + 2^{\frac{n-2}{2}}$

$F(n) \geq 2^{\frac{n}{2}} * 2^{-\frac{1}{2}} + 2^{\frac{n}{2}} * 2^{-1} $

$F(n) \geq 2^{\frac{n}{2}} * ( 2^{-\frac{1}{2}} + 2^{-1} )$

{$2^{-\frac{1}{2}}+2^{-1}$ è sempre maggiore o uguale ad 1}

{$Fib2(n)$ risulta essere troppo poco efficiente}

$T(8)=61$

$T(45) = 3,404,709,508$

{Definiamo allora $Fib3(n)$ utilizzando l'iterazione al posto della ricorsione}

\lstinputlisting{code/fib3.txt}

$T(Fib3_n)=3+(n-2)+(n-1)=2n$

{Ci chiediamo se fib3 sia efficiente dal punto di vista della
memoria\ldots{}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\section{\texorpdfstring{{}}{}}\label{h.t0xc4p18h6wu}

\hypertarget{h.jkxlloc1lefg}{\section{\texorpdfstring{{Notazione
asintotica: le classi O, Omega, Theta
}}{Notazione asintotica: le classi O, Omega, Theta }}\label{h.jkxlloc1lefg}}

{{[}DFI{]} 2.2; {[}CLRS{]} 3.1}

{}

{}

{}

\hypertarget{h.tn5j57miv2l8}{\section{\texorpdfstring{{Esercizi sulla
notazione asintotica. Le classi o,
omega}}{Esercizi sulla notazione asintotica. Le classi o, omega}}\label{h.tn5j57miv2l8}}

{{[}CLRS{]} 3.1}

{}

{}

\hypertarget{h.qp9ilz1tito1}{\section{\texorpdfstring{{Divide et impera.
Il teorema fondamentale delle ricorrenze (o ``Teorema master'').
Esercizi.}}{Divide et impera. Il teorema fondamentale delle ricorrenze (o Teorema master). Esercizi.}}\label{h.qp9ilz1tito1}}

{{[}DFI{]} 2.5; {[}CLRS{]} 4.3}

{}

{}

{}

\hypertarget{h.a5tr7osf4zwh}{\section{\texorpdfstring{{Rappresentazione
di una lista attraverso un vettore
(matrice)}}{Rappresentazione di una lista attraverso un vettore (matrice)}}\label{h.a5tr7osf4zwh}}

{}

{La lista è doppiamente concatenata, ovvero contiene riferimento sia
all'elemento precedente che a quello successivo.}

{}

{La costante NULL viene rappresentata da un indice che non appartiene
all'insieme degli indici del vettore (0 in pseudocodice, -1 in C)}

{}

{\includegraphics{images/image520.png}}

{}

{}

{Analogamente si può creare una lista singolarmente concatenata chiamata FreeList contenente le celle libere (i campi Key e Previous si possono ignorare). Essa verrà utilizzata per l'allocazione di una nuova lista. FreeList è una variabile globale.}

{All'inizio {[}\ldots{}{]} la FreeList contiene TUTTI gli oggetti non allocati.}

{Allocate\_Object()~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}{Complessità $\Theta(1)$}

\lstinputlisting{code/allocate_object.txt}

{\#Inserisce la cella da liberare in testa alla FreeList}

{Free\_Object(x)}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Complessità $\Theta(1)$}

\lstinputlisting{code/free_object.txt}

\section{Alberi}

{{[}CLRS{]} pp. 977-979}

{L'albero è un tipo particolare di grafo connesso, aciclico e non orientato.}

{Un albero radicato è una coppia $T=(N,A)$}

{$N$ è un insieme finito di nodi fra cui si distingue un nodo $R$, detto `Radice'.}

$A${~è un sottoinsieme del prodotto cartesiano $(NxN)$,è un insieme di coppie di nodi che chiamiamo archi.}

{In un albero, ogni nodo $v$ (eccetto la radice) ha esattamente un genitore che viene chiamato padre $u$, tale che $(u,v) \in A$}

{\includegraphics{images/image539.png}}

{Un nodo può avere $0$ o più figli, un figlio è tale se esiste un arco $(u,v) \in A$}

{Il numero dei figli di un nodo si chiama GRADO del nodo.}

{Un nodo senza figli è detto FOGLIA o NODO ESTERNO.}

{Un nodo non foglia è un NODO INTERNO.}

{Se due nodi hanno lo stesso padre, allora sono fratelli.}

{Il cammino da $v$ a $v'$ nell'albero $T$ è una sequenza di nodi $n_0,n_1,\ldots,n_k$ tale che soddisfa queste condizioni:}

$v=n_0$

$v'=n_k$

$(n_{i-i},n_i) \in A\,\forall\,1>i>k$

{La lunghezza di un cammino è il numero di archi nel cammino oppure il numero di nodi diminuito di 1.}

{Sia $x$ un nodo dell'albero radicato $T$ con radice $r$.}

{Qualsiasi nodo $y$ in un cammino, che parte dalla radice $r$ ed arriva ad $x$, è detto ANTENATO di $x$($x$ compreso, è antenato di se stesso).}

{Se $y$ è un antenato di $x$, allora $x$ è DISCENDENTE di $y$}

{NB: Ogni nodo è DISCENDENTE ed ANTENATO di sè stesso.}

{Se $y$ è un antenato di $x$ ed $x$ è diverso da $y$, allora $y$ è un ANTENATO PROPRIO di $x$ ed $x$ è DISCENDENTE PROPRIO di $y$.}

{Il sottoalbero con radice in $x$ è l'albero indotto dai discendenti di $x$}

{La profondità di un nodo $x$ è la lunghezza del cammino dalla radice ad $x$.}

{Un livello di un albero è costituito da tutti i nodi che stanno alla stessa profondità. }

{L'altezza di un nodo $x$ è la lunghezza del più lungo cammino che scende da $x$ alle foglie (qualsiasi foglia) di profondità massima.}

{L'altezza di un albero è l'altezza del nodo radice.}

{L'altezza è la massima profondità di un qualsiasi nodo dell'albero.}

\subsection{Alberi binari}

{Gli alberi binari sono definiti in modo ricorsivo:}

\begin{itemize}
\tightlist
\item
  {Un albero vuoto è binario}
\item
  {Un albero costituito da un nodo (radice) ,}
\end{itemize}

{~~~~~~~~~~~~~~~~da un albero binario detto sottoalbero sinistro della
radice,}

{~~~~~~~~~~~~~~~~e da un'altro albero binario detto sottoalbero destro
della radice}

{~~~~~~~~~~~~~~~~è detto un albero binario.}

\subsection{Albero k-ario}

{E' un albero in cui i figli si un nodo sono etichettati con interi positivi distinti e le etichette maggiori di $K$ sono assenti = ogni nodo può avere al più $K$ figli.}

{Un albero K-ario completo è tale quando tutte le foglie hanno la stessa profondità e tutti i nodi interni hanno grado $K$}

{ALGORITMO}

{Trovare l'algoritmo per determinare la completezza di un albero}

{Dimostro per induzione che le foglie sono $K^h$}


{$h=0$ (caso base): $K^0=1$, OK}

{Assumiamo che per un albero di altezza $h$ sia vero che $n=K^h$}

{Lo dimostro per $h+1$}

{Il numero di nodi di profondità $h$ è $K^h$ per ipotesi}

{Il numero di nodi di profondità $h+1$ è $K^h*K=K^{h+1}$, OK}

\paragraph{ALGORITMO}

{Trovare il numero di foglie e il numero di nodi interni di un albero k-ario completo di altezza $h$}

{$\sum_{\varphi=0}^{h-1}{K^{\varphi}}$ è una sommatoria GEOMETRICA, si semplifica in $\frac{K^{h-1+1}-1}{K-1}$ con $k\neq 1$, quindi $\frac{K^h-1}{K-1}$}

{Altezza di un albero k-ario completo con $n$ nodi:}

$n=K^h \rightarrow log_k(n) = log_k(K^h) \rightarrow log_k(n) = h$

\paragraph{Proprietà}

{Dimostrare per induzione che in un albero BINARIO completo non nullo avente $n$ nodi, il numero di foglie è $\frac{n+1}{2}$}

\subsubsection{Tipo di dato ALBERO}

{Struttura:}

{Insieme di nodi}

{Insieme di archi}


{Le operazioni del tipo ALBERO:}


{~~~~~~~~newTree() → T~~~~~~~~~~~~~~~~Nuovo albero T}

{~~~~~~~~numNodi(T) → int~~~~~~~~~~~~~~~~Numero di nodi presenti in T}

{~~~~~~~~grado(T,N) → int~~~~~~~~~~~~~~~~Numero di figli di N, N
appartenente a T}

{~~~~~~~~padre(T,N) → int~~~~~~~~~~~~~~~~Nodo padre, null se N è radice,
~N appartenente a T}

{~~~~~~~~figli(T,N) → int{[}{]}~~~~~~~~~~~~~~~~Lista contenente i figli
del nodo N, N appartenente a T}

{}

\section{Rappresentazione di alberi tramite array}

\subsection{Utilizzo del vettore padri}

{Sia $T=(N,A), N = \{n_1,n_2,\ldots,n_n\}$}

{Costruisco un vettore di dimensione $n$, le cui celle contengono copie di $(i,u)$. Con $i$ indichiamo l'informazione del nodo e con $u$ indichiamo il nodo padre (indice).}

$\forall v \in [1,n]$

{p{[}v{]} → info = contenuto}

{p{[}v{]} → padre = indice del padre,$(u,v) \in A(archi)$, se $v$ è radice il padre è $NULL/-1/0$}

{Spazio per memorizzare $n=\Theta(n)$}

\subsubsection{Implementazioni}

{Padre - Complessità $\Theta(1)$}

\lstinputlisting{code/tree_vecp_padre.txt}

{Figli - Complessità $\Theta(n)$}

\lstinputlisting{code/tree_vecp_figli.txt}

\subsection{Utilizzo del vettore posizionale}

{L'albero deve essere completo e con $k \geq 2$. La ricerca è ottimizzata rispetto all'albero dei padri. Ogni nodo ha una posizione prestabilita.}

{Utilizzo un vettore posizionale $P$ di dimensione $n$ tale che }

\begin{enumerate}
\tightlist
\item
  {0 è la posizione della radice}
\item
  {l'i-esimo figlio di un certo nodo $v$ è in posizione $kv+i+1$, con $0\leq i < K-1$}
\end{enumerate}

{un nodo $f$ è foglia se non ha figli, quindi se $kv+1>n$}

{Il padre del nodo $f$ è in posizione $\floor{\frac{f-1}{k}}$ (parte intera inferiore) e i figli si trovano tra $kv+1$ e $kv+1+i-1$.

\subsubsection{Implementazioni}

{Padre - Complessità $\Theta(1)$}

\lstinputlisting{code/tree_vec_padre.txt}

{Figli - Complessità $\Theta(k)$, con $k$ = grado di $v$}

\lstinputlisting{code/tree_vec_figli.txt}

\subsection{Utilizzo di strutture collegate}

\paragraph{Parent + Childs}

{}

{Ogni nodo è un record con i seguenti campi:}

{~~~~~~~~k : informazione}

{~~~~~~~~p : puntatore al padre}



{~~~~~~~~Se il numero di figli è noto}

{}

{~~~~~~~~~~~~~~~~left : puntatore al figlio sinistro}

{~~~~~~~~~~~~~~~~right : puntatore al figlio destro}

{}

{Altrimenti si utilizza una lista di puntatori ai propri figli}

{c {[}{]} : lista di puntatori ai figli~~~~~~~~}

{Se ogni nodo ha grado al più $k$, è possibile mantenere in ogni nodo un puntatore a ciascuno dei possibili $k$ figli}

{Spazio necessario: $\Theta(nk)$, se $k$ costante allora $\Theta(n)$}

\subsubsection{Implementazioni:}

{Padre - Complessità $O(1)$}

\lstinputlisting{code/tree_pc_padre.txt}

{Figli - Complessità $O(k)$, con $k$ = grado di $v$ }

{Con $k$ non noto si utilizza un ciclo che scorre c{[}{]}}

\lstinputlisting{code/tree_pc_figli.txt}

\paragraph{Parent + Left child + Right sibling}

{Ogni nodo è un record con i seguenti campi:}

{~~~~~~~~k : informazione}

{~~~~~~~~p : puntatore al padre}

{~~~~~~~~left\_child : puntatore al figlio sinistro}

{~~~~~~~~right\_sibling : puntatore al fratello immediatamente a destra}

\subsubsection{Implementazioni}

{Padre - Complessità $\Theta(1)$}

\lstinputlisting{code/tree_plcrs_padre.txt}

{Figli - Complessità $\Theta(k)$ con $k$ = grado di $v$}

\lstinputlisting{code/tree_plcrs_figli.txt}

\section{Algoritmi di visita degli Alberi}

{{[}DFI{]} pp. 77-80}

\subsubsection{Visita generica}

\lstinputlisting{code/visita_generica.txt}

{Dimostrare che ha costo LINEARE}\textsuperscript{\protect\hyperlink{cmnt2}{{[}b{]}}}

\hypertarget{h.6xasx7f8zgn1}{\paragraph{\texorpdfstring{{Teorema}}{Teorema}}\label{h.6xasx7f8zgn1}}

{L'algoritmo di visita, applicato alla radice di un albero con $m$ nodi, termina in $O(m)$ iterazioni. Lo spazio usato è $O(n)$}

\hypertarget{h.zdc8liauzt1c}{\paragraph{\texorpdfstring{{Dimostrazione}}{Dimostrazione}}\label{h.zdc8liauzt1c}}

{Hp: L'inserimento e la cancellazione da $S$ sono effettuati in tempo costante}

{Ogni nodo verrà inserito ed estratto dall'insieme $s$ una sola volta, perchè in un albero non si può tornare ad un nodo a partire dai suoi figli.}

{Quindi le iterazioni del ciclo while saranno al più $O(n)$}

{Poiché ogno nodo compare al più una volta in $S$, lo spazio richiesto non è più alto di $O(n)$}

\subsubsection{Visita DFS - Depth first search - Ricerca in profondità}

{Seguiamo tutti i figli sinistri, andando in profondità fino a che non si raggiunge la prima foglia sinistra. Solo quando il sottoalbero sx è stato completamente visitato, si passa ~a visitare il sottoalbero dx}

\lstinputlisting{code/dfs_visit.txt}

\subsubsection{Visita BFS - Breadth first search - Ricerca in ampiezza}

\section{Heap}

{Heap: albero quasi completo con tutti i nodi dell'ultimo livello a sinistra}

{MaxHeap: La radice ha valore maggiore o uguale a quello dei figli}

{MinHeap: La radice ha valore minore o uguale a quello dei figli}

\subsubsection{Lemma 2}

{Nell'array che rappresenta un heap di $n$ elementi, le foglie sono i nodi con indici che vanno alle posizioni}

$\frac{n}{2}+1,\frac{n}{2}+2,\ldots,n$

\includegraphics{images/image526.png}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
Elemento & 16 & 15 & 10 & 14 & 7 & 9 & 3 & 2 & 8 & 1 \\ 
\hline 
Posizione & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
\hline 
 &  &  &  &  & $\frac{n}{2}$ & $\frac{n}{2}+1$ & $\frac{n}{2}+2$ & $\frac{n}{2}+3$ & $\frac{n}{2}+4$ & $n$ \\ 
\hline 
\end{tabular} 

\subsubsection{Lemma 3}

{Altezza di un nodo: cammino più lungo verso una foglia}

{Il teorema definisce la numerosità di nodi con una certa altezza:}

{Ci sono al massimo $\frac{n}{2^{h+1}}$ nodi di altezza $h$ in
un qualsiasi heap di $n$ elementi}

\subsubsection{Max\_heapify}}}

{L'operazione max\_heapify permette di mantenere le proprietà di maxheap}

{Precondizioni:}

{Gli alberi binari con radice in left(i) e right(i) sono maxheap}

{Postcondizioni:}

{~~~~~~~~L'albero radicato in $i$ è un maxheap}

\lstinputlisting{code/max_heapify.txt}

{Tempo di esecuzione : $O(h)$ dove $h$ è l'altezza del nodi i poichè l'heap ha altezza $log(n)$ (LEMMA 2)}

\subsection{Dato un vettore disordinato, costruire un heap}

\lstinputlisting{code/build_maxheap.txt}

{Invariante: ogni nodo in posizione $i+1,\ldots,n$ è radice di un maxheap, con n = MANCA}

{Sembrerebbe complessa $O(\frac{n}{2}\,log(n)) = O(n\,log(n))$ ma max\_heapify lavora principalmente su foglie, quindi la complessità è lineare $O(n)$.}

\begin{equation}
\sum_{h=0}^{log(n)}{\floor*{\frac{h}{2^{h-1}}}*O(h)} = O(n*\sum_{h=0}^{log(n)}{\floor*{\frac{h}{2^h}})}
\end{equation}

{L'ultima sommatoria è la serie nota}

\begin{equation}
\sum_{h=0}^{+\infty}{h*x^h} = \frac{x}{{(1-x)}^2}
\end{equation}

{Con $x=\frac{1}{2}$:}

\begin{equation}
\sum_{h=0}^{+\infty}{h*x^h} = \frac{\frac{1}{2}}{{(1-\frac{1}{2})}^2} = 2
\end{equation}

{Quindi}

\begin{equation}
O(n*\sum_{h=0}^{log(n)}{\floor*{\frac{h}{2^h}}}) = O(2n) = O(n)
\end{equation}

\subsection{Heapsort}

\lstinputlisting{code/heapsort.txt}

{INV = Il sottoarray che va dalla posizione $1$ alla posizione $i$ è un maxheap che contiene gli elementi più piccoli del intero vettore di partenza, mentre $A[a+1,\ldots,n]$ contiene gli $n-1$ elementi più grandi di $A[1..n]$ ordinati}{.}

\subsubsection{Teorema}

{L'algoritmo HeapSort ordina in loco $n$ elementi eseguendo nel peggiore dei casi $O(nlog(n))$ confronti in quanto algoritmo basato sui confronti.}

\hypertarget{h.jih9riph7gns}{\subsection{\texorpdfstring{{Code di
priorità}}{Code di priorità}}\label{h.jih9riph7gns}}

{{[}CLRS{]} pp. 135-140}

{Struttura dati che serve a mantenere un insieme dinamico i cui elementi ( aggiungibili e rimovibili ) hanno un valore associato detto chiave o peso.}

{Esistono due tipi di code di priorità:}

\begin{itemize}
\tightlist
\item
  {MaxPriorità (Si utilizza la struttura MaxHeap)}
\item
  {MinPriorità (Si utilizza la struttura MinHeap)}
\end{itemize}

{Le operazioni delle code di priorità massima/minima sono:}

{Insert( Coda s , Elemento X) : Inserisce l'elemento $X$ in $S$}

{Maximum( Coda s ): Restituisce l'elemento di $S$ con la chiave maggiore senza rimuoverlo}

{Minimum( Coda s) : Restituisce l'elemento di $S$ con la chiave minore senza rimuoverlo}

{Extract\_max( Coda s) : Elimina e restituisce l'elemento di $S$ con la chiave maggiore}

{Extract\_min( Coda s) : Elimina e restituisce l'elemento di $S$ con la chiave minore}

{Increase\_key( Coda s, Elemento x, Chiave k) : Incrementa il valore della chiave di $X$ al nuovo valore $K$. Si suppone che $K$ sia maggiore o uguale al valore corrente della chiave di $X$, $K \geq chiave(X)$}

{Decrease\_key( Coda s, Elemento x, Chiave k) : Decrementa il valore della chiave di $X$ al nuovo valore $K$. Si suppone che $K$ sia minore o uguale al valore corrente della chiave di $X$, $K \leq chiave(X)$}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Implementazione di code di massima priorità con le strutture Heap}}

\lstinputlisting{code/heap_maximum.txt}

{Complessità di Heap Maximum : $O(1)$}

\lstinputlisting{code/heap_extract_max.txt}

{Complessità di Heap Extract Max : $O(log(n))$}

\lstinputlisting{code/heap_increase_key.txt}

{Complessità di Heap Increase Key : $O(log(n))$}

{Invariante del while: L'array $A[i,\ldots,A.heapSize]$ soddisfa le proprietà di maxHeap tranne una possibile violazione: A{[}i{]} potrebbe essere più grande di A{[}parent(i){]}}

{Con un Heap di n elementi, la complessità è $O(log(n))$ in quanto il cammino dal nodo fino alla radice ha lunghezza }$O(log(n))$

\lstinputlisting{code/heap_insert.txt}

{Complessità di Heap\_Insert : }$O(log(n))$

{La ricerca su una cosa di priorità, nel caso peggiore, ha costo }$O(n)$

{Premessa : $1 \leq i \leq A.heapSize$}

\lstinputlisting{code/heap_delete.txt}

{Complessità di Heap Delete : }$O(log(n))$

\hypertarget{h.r8bopy9q4g8f}{\subsubsection{\texorpdfstring{{Esercizi}}{Esercizi}}\label{h.r8bopy9q4g8f}}

{Esercizio 1 }{: }

{Scrivere una funzione che determini se un albero è quasi completo. Gli output devono essere 1 se l'albero è quasi completo o 0 se non lo è. }{L'albero è rappresentato con notazione left e right.}

{Note: notiamo che non è sufficiente sapere se un albero è quasi completo o meno, necessitiamo anche di un valore per rappresentare l'albero completo.}

{Gli output saranno quindi: 0 - albero completo, 1 - albero quasi completo, 2 - albero non quasi completo.}

\lstinputlisting{code/is_quasi_completo.txt}

\lstinputlisting{code/is_quasi_completo_aux.txt}

{Complessità di is\_quasi\_completo : $O(n)$}

$T(n) = T(k) + T(n-k-1) + c = O(n)$

\paragraph{Esercizio 2}

{Siano dati due alberi binari completi di radice $r$ ed $s$ aventi la stessa altezza $h$ e dimensione totale (somma dei nodi dei due alberi) $n$. Le chiavi memorizzate nei nodi soddisfano la proprietà di maxheap. Si vuole creare un unico albero quasi completo maxheap, fusione dei due alberi, con altezza $h+1$ e dimensione $n$.}

{Le seguenti soluzioni vengono accettate:}

\begin{enumerate}
\tightlist
\item
  {Soluzione di costo e tempo $\Theta(n)$ e in spazio aggiuntivo $\Theta(n)$}
\item
  {Soluzione (più elegante) di costo e tempo $O(log(n))$ e spazio aggiuntivo costante}
\end{enumerate}

{Soluzione :}

\begin{enumerate}
\tightlist
\item
  {Soluzione di costo e tempo $\Theta(n)$ e in spazio aggiuntivo $\Theta(n)$}
\end{enumerate}

{~~~~~~~~~~~~~~~~~~~~~~~~Vettore di n elementi v. (spazio aggiuntivo $\Theta(n)$)}

{~~~~~~~~~~~~~~~~~~~~~~~~Carichiamo il vettore con i valori di $r$ ed $s$. $\Theta(n)$}

{~~~~~~~~~~~~~~~~~~~~~~~~Applichiamo build\_maxheap all'intero vettore $v$. $\Theta(n)$}

{~~~~~~~~~~~~~~~~~~~~~~~~Creo l'albero corrispondente. $\Theta(n)$}

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  {Soluzione di costo e tempo $O(log(n))$ e spazio aggiuntivo costante}
\end{enumerate}

{Non possiamo fare altro che modificare le strutture a nostra disposizione, non usiamo strutture ausiliarie.}

{Prendo come radice il nodo foglia $x$ più a destra dell'albero $s$. $\Theta(log(n))$}}

{Assegno $x.left$ ad $s$ e $x.right$ ad $r$.}

{Applico la max\_heapify al nuovo nodo radice x. $\Theta(log(n))$}


\section{Limite inferiore per l'ordinamento per confronti}

{{[}CLRS{]} pp. 157-159}

{Fino ad ora abbiamo visto i seguenti algoritmi di ordinamento, tutti basati sul confronto.}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{MergeSort}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
$\Theta(n\,log(n)))$\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{QuickSort}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{Caso medio : $\Theta(n\,log(n)))$, caso pessimo : $\Theta(n^2)$}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{HeapSort}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
$\Theta(n\,log(n)))$\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
{InsertionSort}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
$\Theta(n^2)$\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

{Ci domandiamo, è possibile infrangere il limite inferiore di $\Theta(n\,log(n))$? Dimostreremo che NON è possibile farlo con algoritmi basati sul confronto.}

{Analizziamo il limite inferiore degli algoritmi basati sul confronto:}

{$\Omega(n)$ è il limite banale, in quanto devo analizzare n elementi. Questo limite è tuttavia irrealistico e insensato.}

{Tutti gli algoritmi basati sul confronto hanno come limite inferiore $\Omega(n\,log(n))$. Per dimostrarlo facciamo uso degli alberi di decisione.}

{Un albero di decisione è un'astrazione di un qualsiasi algoritmo di ordinamento basato sui confronti.}

\subsection{Esempio: Ordina 3 elementi}

{\includegraphics{images/image531.png}}

{Questa struttura assomiglia all'InsertionSort}\textsuperscript{\protect\hyperlink{cmnt15}{{[}o{]}}}

{Per un input $A = <A_1,\ldots,A_n>$ di dimensione $n$, ogni nodo interno è etichettato da una coppia $i:j$ dove $i,j$ sono indici dell'insieme da ordinare.}

\begin{itemize}
\tightlist
\item
  {$i:j$ significa confrontare $A_i$ con $A_j$}
\item
  {il sottoalbero sinistro dà i successivi confronti se $A_i \leq A_j$}
\item
  {il sottoalbero destro dà i successivi confronti se $A_i > A_j$}
\item
  {ogni foglia fornisce una permutazione dell'input tale che se io vado a prendere gli elementi dell'input, ordinati a seconda della permutazione, ottengo l'insieme ordinato in ordine crescente}
\end{itemize}

{Dato un qualsiasi algoritmo di ordinamento basato sul confronto, è possibile costruirmi gli alberi di decisione.}

\begin{itemize}
\tightlist
\item
  {Ogni valore di n è associato ad un proprio albero di decisione.}
\item
  {L'albero modella tutte le tracce possibile di esecuzione.}
\item
  {Il tempo di esecuzione (cioè il numero di confronti necessari) è la lunghezza di un cammino sull'albero.}
\item
  {Il tempo di esecuzione nel caso peggiore è il più lungo cammino dell'albero, ovvero l'altezza dell'albero.}
\end{itemize}

{Mi basta quindi trovare la dimensione dell'albero di decisione per
determinare il tempo di esecuzione.}

\subsection{Quanto è grande un albero di decisione?}

\subsubsection{Quante foglie contiene?}

{L'albero ha come minimo $n!$ foglie poichè, per essere corretto, ogni permutazione deve comparire almeno una volta.}

\paragraph{Lemma 2}

{Un albero binario di altezza h ha al più $2^h$ foglie.}

{Dimostrazione per induzione:}

\begin{itemize}
\tightlist
\item
  {Se $h=0$ abbiamo un albero costituito da un solo nodo radice che è l'unica foglia. Ovviamente con $h=0$, $1 \leq 2^h$ è verificata, infatti $1\leq 2^0$.}
\item
  {Assumiamo vera la proprietà per alberi binari di altezza $k<h$ e lo dimostro per $h$. Sia $r$ la radice dell'albero $t$. }
\end{itemize}

\begin{itemize}
\tightlist
\item
  {Se $t$ ha un solo figlio allora il numero di foglie di $t$ è uguale a quello del figlio che ha altezza $h-1$. }{Per ipotesi induttiva}{: Il numero delle foglie del sottoalbero figlio è minore di $2^{h-1}$, che è minore di $2^h$.}
\item
  {Se sono presenti entrambi il figlio sx e il figlio dx, allora il numero delle foglie è dato dalla somma delle foglie dei due sottoalberi. Siano $h_L,h_R$ le altezze dei due figli. Entrambe sono minori di $h$.}
\end{itemize}

\begin{equation}
f=f_L+f_R = 2^{h_L} + 2^{h_R} \leq 2 * 2^{max(h_L,h_R)}
\end{equation}

{$f \leq 2^{1+max(h_L,h_R)}$ ma $1+max(h_L,h_R) = h$, quindi $f \leq 2^h$}

{Quindi il numero delle foglie è compreso tra $n!$ e $2^h$.}

\paragraph{Teorema:}

{Qualsiasi algoritmo di ordinamento per confronti richiede almeno $\Omega(n\,log(n))$ confronti nel caso peggiore.}

{Dimostrazione:}

{Bisogna determinare l'altezza di un albero di decisione, dove ogni permutazione appare come foglia. Si consideri un albero di decisione di altezza $h$ con $l$ foglie che corrisponde ad un ordinamento per confronti di $n$ elementi. }

{Allora $n! \leq l \leq 2^h$ (per Lemma 2)}

{Passando al logaritmo, $h \geq log(n!)$}

{Utilizziamo l'approssimazione di \href{https://www.google.com/url?q=https://it.wikipedia.org/wiki/Approssimazione_di_Stirling\&sa=D\&ust=1523379128517000}{Stirling} per approssimare $n!$:}

\begin{equation}
n! \simeq \sqrt{2\pi n }* {(\frac{n}{e})}^n
\end{equation}

{Per n sufficientemente grande, considero solo il termine più grande}

$h \geq log({(\frac{n}{e})}^n)$

{Per proprietà dei logaritmi}

$h \geq n*log(\frac{n}{e})$

{$h \geq n*(log(n) - log(e))$ , il secondo algoritmo è costante}

$n \geq n * log(n)$

{Non ci può essere quindi un algoritmo basato sui confronti minore di $n * log(n)$}

\paragraph{Corollario:}

{Gli algoritmi HeapSort e MergeSort sono algoritmi di ordinamento per confronti asintoticamente ottimali.}

{Dimostrazione:}

{I limiti superiori dei sue algoritmi sono $O(n\,log(n))$ nei tempi di esecuzione corrispondono al limite inferiore $\Omega(n\,log(n))$ nel caso peggiore dato dal teorema.}

{Se elimino il modello basato sui confronti, posso battere il limite inferiore di $\Omega(n\,log(n))$? SI.}

\subsection{Algoritmi di ordinamento privi di confronti}

\subsubsection{CountingSort}

{{[}CLRS{]} pp. 159-161}

{Assunzione:}

{~~~~~~~~I numeri da ordinare sono interi in un intervallo che va da 0 a $k$, per qualche $k$ prefissato}

{Input:}

{$A=[0,\ldots,n]$ dove $A[j] \in [0,\ldots,k]$, $n,k$ sono parametri}

{Output:}

{$B=[0,\ldots,n]$ ordinato, permutazione di A}

{Utilizzo una memoria ausiliaria $C$, costituita da $k+1$ elementi, $C=[0,\ldots,n]$}

{Codice:}

\lstinputlisting{code/counting_sort.txt}

{Complessità di CountingSort : $\Theta(n+k)$}

{Di solito il CountingSort quando $k$ è limitato superiormente da n, $k=O(n)$. Allora il tempo di esecuzione risulta $\Theta(n)$. Il tempo di esecuzione di questo algoritmo è dato da
$\Theta(n+k)$. Il CountingSort di solito è utilizzato quando $k$ è limitato superiormente da $n$ in $k=O(n)$. Allora, il tempo di esecuzione risulta essere $O(n)$.}

{Il CountingSort è un algoritmo stabile, caratteristica molto importante.}

{Analisi:}

{~~~~~~~~All'uscita dal secondo ciclo: $C[i] = \abs{\{x \in \{1\ldots n\}\,|\,A[x] = i\}}$}

{~~~~~~~~All'uscita dal terzo giro: $C[i] = \abs{\{x \in \{1\ldots n\}\,|\,A[x] \leq i\}}$}


\subsubsection{RadixSort}

{{[}CLRS{]} pp. 162-164}

{\includegraphics{images/image540.png}}

{L'algoritmo di ordinamento scelto deve essere stabili e rispettare l'ordine.}

\lstinputlisting{code/radix_sort.txt}

{Dimostriamo la correttezza dell'algoritmo tramite induzione sulla colonna da ordinare:}

{Caso base (i=1)}

{~~~~~~~~Ordino l'unica colonna}

{Assumo che le cifre delle colonne che vanno da $1$ a $i-1$ siano ordinate}

{Dimostro che un algoritmo stabile sulla colonna $i$, lascia le colonne da $1$ a $i$ ordinate:}

{Se due cifre in posizione $i$}

\begin{itemize}
\tightlist
\item
  {sono uguali: per stabilità, rimangono nello stesso ordine e, per ipotesi induttiva, sono ordinate.}
\item
  {sono diverse: l'algoritmo di ordinamento sulle colonna $i$ le ordina e le mette in posizione corretta.}
\end{itemize}

{Complessità: $\Theta(d*(k+n))$}

{Teorema: }

{Dati $n$ numeri di $d$ cifre, dove ogni cifra può avere fino a $k$ valori possibili}\textsuperscript{\protect\hyperlink{cmnt16}{{[}p{]}}}{, la procedura ordina correttamente i numeri nel tempo di $\Theta(d*(k+n))$ se l'algoritmo stabile utilizzato dalla procedura impiega un tempo $\Theta(k+n)$}

{Dimostrazione:}

{Per ogni iterazione, il costo risulta essere $\Theta(k+n)$}

{Le iterazioni sono $d$, per un totale di $\Theta(d*(k+n))$}

{Osservazioni:}

{~~~~~~~~Se $k=O(n)$ il tempo di esecuzione è $\Theta(d*n)$}

{Inoltre, se $d$ è costante, la complessità è $\Theta(n)$}

\paragraph{Come ripartire le chiavi in cifre?}

\begin{itemize}
\tightlist
\item
  {Usiamo il CountingSort su ciascuna cifra}\includegraphics{images/image198.png}
\item
  {Siano }$n${~interi, ognuno costituito da $b$ bits}
\item
  {Divido ogni intero in $\ceil*{\frac{b}{r}}$ ``cifre'', ognuna di $r$ bits. La cifra appartiene a $[0,\ldots,2^r-1]$, $k=2^r$}
\end{itemize}

{Esempio: Parola di 32 bits}

{La suddivido in cifre, ciascuna di 8 bits}

\includegraphics{images/image204.png}

\includegraphics{images/image205.png}

\includegraphics{images/image206.png}

{Cerco di minimizzare la complessità ponendo $r$ grande, $\frac{b}{r}*n$ risulta diminuito, ma $\frac{b}{r}*2^r$ è cresciuto esponenzialmente. Scelgo $r$ piccolo, altrimenti $2^r$ domina su $n$.}

{Scegliamo $r$ essere il massimo valore tale che $n$ risulti essere $n\geq 2^r$, quindi $r=log(n)$.}

{Sostituendo:}

\includegraphics{images/image212.png}{, essendo $2^{log(n)}$, abbiamo }\includegraphics{images/image214.png}

{I numeri variano nell'intervallo $[0,\ldots,2^b-1]$. Se fisso $b=c*log(n)$, allora l'intervallo diventa $[0,\ldots,n^c-1]$.}

{Allora il tempo è uguale a $\Theta(c*n)$, se $c$ è costante, allora il tempo è $\Theta(n)$.}

{Ho ampliato la grandezza dell'intervallo su cui posso applicare l'algoritmo.}

\section{Tabelle hash}

{{[}CLRS{]} pp. 209-211}

{Le tabelle hash sono una possibile implementazione dei dizionari,insiemi dinamici con inserimento, cancellazione e ricerca dove ogni elemento è associato ad una chiave.}

{I dizionari possono essere implementati nei seguenti modi:}

\begin{itemize}
\tightlist
\item
  {Liste: $O(n)$}
\item
	{Alberi binari}
	
\begin{itemize}
\tightlist
\item
  	{Non bilanciati: $O(n)$}
\item
  	{Bilanciati: $O(log(n))$}
\end{itemize}

\item
  	{Tabelle hash}
  	
\begin{itemize}
\tightlist
\item
	{Tempo medio: $\Theta(1)$ (Motivo principale per l'utilizzo delle tabelle hash)}
\item
	{Caso peggiore: $\Theta(n)$}
\end{itemize}

\end{itemize}

{Vogliamo fornire un'applicazione che ha bisogno di un insieme dinamico (insert/delete/search). Ogni elemento ha una chiave estratta da un universo $\mathbb{U}$ con $\abs{\mathbb{U}} = \{0,\ldots,w-1\}$, dove $w$ non è troppo grande. Nessun elemento ha la stessa chiave (elementi distinti). }

{\includegraphics{images/image521.png}}

{Mi costruisco una tabella ad accesso diretto. Si può utilizzare un array $[0,\ldots,w-1]$ dove}

\begin{itemize}
\tightlist
\item
  {ogni posizione (o cella) corrisponde ad una chiave di $\mathbb{U}$}
\item
  {Se c'è un elemento $x$ con chiave $k$, allora nella posizione $T(k)$ è contenuto un puntatore a $x$}
\item
  {Altrimenti, se l'insieme non contiene l'elemento, $T(k) = NULL$}
\end{itemize}

\begin{equation}
T(k) = 
\begin{cases}
x & \mbox{se } x.key=k, x\in K \\ 
NULL & \mbox{altrimenti} 
\end{cases}
\end{equation}

{Direct\_Access\_Search : }$O(1)$

\lstinputlisting{code/direct_access_search.txt}

{Direct\_Access\_Insert : $O(1)$}

\lstinputlisting{code/direct_access_insert.txt}

{Direct\_Access\_Delete : $O(1)$}

\lstinputlisting{code/direct_access_delete.txt}

{Potrebbe essere uno spreco se $\abs{K} \ll \abs{\mathbb{U}}$ (Molto minore)}

{Pregi:}

{Operazioni eseguibili in tempo costante}

{Difetti:}

{Lo spazio utilizzato è proporzionale a $w$ e non al numero $n$ di elementi. Di conseguenza si può avere un importante spreco di memoria.}

{Per ovviare al problema dello spreco di memoria si utilizzano le tabelle hash.}

{Richieste:}

\begin{enumerate}
\tightlist
\item
  {Vogliamo ridurre lo spazio per la tabella ad $\Theta(\abs{K})$, ovvero il numero di chiavi
  effettivamente utilizzate}
\item
  {Le operazioni siano con costo medio $\Theta(1)$, ma non nel caso pessimo}
\end{enumerate}

{Idea:}

{Invece di memorizzazione un elemento con chiave o nella cella $k$, si usa una funzione $h$, detta funzione hash, e si memorizza l'elemento nella cella $h(k)$}


{$h:\mathbb{U}\rightarrow\{0,1,\ldots,m-1\}$, dove $m$ della tabella hash è generalmente molto più piccola della dimensione dello spazio di tutte le chiavi.}

{Problema: }

{Se $m < \abs{\mathbb{U}}$, due valori possono essere mappati nella stessa cella, ciò è chiamato collisione. }

{Le tabelle hash possono soffrire del problema delle collisioni: quando un elemento da inserire è mappato, tramite $h$, in una cella già occupata, si verifica una collisione.}

$k_1 \neq k_2 \in \mathbb{U}, h(k_1) = h(k_2)$

{Se $\abs{K} > m$, ho la certezza di imbattermi in collisioni.}

{Cerchiamo quindi strategie per gestire le collisioni, ne analizzeremo due:}

\begin{enumerate}
\tightlist
\item
  {Metodo di concatenamento (liste di collisione (o di trabocco))}
\item
  {Indirizzamento aperto}
\end{enumerate}

\subsection{Risoluzione delle collisioni tramite metodo di concatenamento}

{Si mettono tutti gli elementi che sono associati alla stessa cella in una lista concatenata.}

{La cella $j$ contiene un puntatore alla testa di una lista di tutti gli elementi memorizzati che sono mappati in $j$. Se non ci sono elementi, la cella $j$ conterrà $NULL$.}

\subsubsection{Implementazioni}

{Andiamo ad analizzare l'hashing con concatenamento.}

\lstinputlisting{code/chained_hash_insert.txt}

{Tempo di esecuzione: $\Theta(1)$ se $h$ impiega tempo costante $\Theta(1)$ e l'elemento non è presente nella lista}

\lstinputlisting{code/chained_hash_search.txt}

{Tempo di esecuzione:}

{~~~~~~~~Caso peggiore:}

{~~~~~~~~~~~~~~~~Proporzionale alla lunghezza della lista nella cella $h(x)$}

\lstinputlisting{code/chained_hash_delete.txt}

{Tempo di esecuzione:}

{Caso peggiore:}

{$\Theta(1)$ se la lista doppiamente concatenata (ci serve un puntatore al predecessore)}

{Senza una lista doppiamente concatenata servirebbe ricercare la chiave del predecessore come ulteriore step.}

{Sia T una tabella hash con n celle dove sono stati memorizzati $n$ elementi. }

{Caso peggiore:}

{Tutti gli elementi sono mappati nella stessa cella. Abbiamo quindi
un'unica lista di lunghezza $n$.}

{Tempo di esecuzione della ricerca: $\Theta(n)$}

{Caso medio:}

{Analisi di $h$}

{Deve soddisfare la proprietà di }{Hashing Uniforme Semplice:}

{Ogni elemento ha la stessa probabilità di essere mandato in una qualsiasi delle $m$ celle, indipendentemente dalle celle in cui sono mancati gli altri elementi.}

$\forall i \in \{0,\ldots,n-1\}, Q(i) = \frac{1}{n}$

{Assumo che $h$ soddisfi la proprietà di Hashing Uniforme Semplice: indico con $n_j$ la lunghezza della lista $T[j]$}

{Il valore medio di $n_j$ è $\alpha=\frac{n_0+n_1+\ldots+n_{m-1}}{m}=\frac{n}{m}$}

{Fattore di carico: }

{In una tabella hash con $n$ chiavi ed $m$ celle, il fattore di carico è $\alpha = \frac{n}{m}$. Esso è il numero medio di elementi memorizzati in ogni lista.}

\paragraph{Teorema 1}

{In una tabella hash in cui le collisioni sono risolte con il concatenamento, una ricerca senza successo richiede, nel caso medio, un tempo $\Theta(1+\alpha)$, nell'ipotesi di Hashing Uniforme Semplice.}

{Intuizione:}

\begin{itemize}
\tightlist
\item
  {Calcolo $i=h(k)$: $\Theta(1)$}
\item
  {Accedo a $T[j]$ : $\Theta(1)$}
\item
  {Scorro la lista $T[j]$ fino alla fine) : $\Theta(\alpha)$ (in media)}
\end{itemize}

\paragraph{Teorema 2}

{In una tabella hash in cui le collisioni sono risolte con il concatenamento, una ricerca con successo richiede, nel caso medio,un tempo $\Theta(1+\frac{\alpha}{2}) = \Theta(1+\alpha)$ nell'ipotesi di Hashing Uniforme Semplice.}

{Se il numero di celle della tabella hash è almeno proporzionale al numero di elementi da memorizzare, cioè abbiamo $n=O(m)$, di conseguenza \\ $\alpha=\frac{n}{m} = \frac{O(m)}{m}=\Theta(1)$. (Se alpha è costante) }

Tutte le operazioni quindi, mediamente, sono svolte in tempo $\Theta(1)$.


\subsubsection{Come si costruiscono le funzioni hash?}

{Significato di hash: polpetta, tritare.}

{Una buona funzione hash dovrebbe essere in grado di distribuire in modo
uniforme le chiavi nello spazio degli indici della tabella. Deve quindi
rispettare l'ipotesi di HUS.}

{Esempio: la distribuzione delle chiavi è nota.}

{Le chiavi sono numeri reali $k$ casuali e distribuiti in modo indipendente e uniforme nell'intervallo $0 \leq k \leq 1$.}

{Per distribuirle su $m$ celle possiamo moltiplicarle per il valore di quest'ultimo: $h(k) = \floor{km}$}

{Questa funzione soddisfa l'ipotesi HUS.}

{Le distribuzioni delle chiavi difficilmente risultano note a priori. }

{Metodi di costruzione delle funzioni hash.}

\begin{enumerate}
\tightlist
\item
  {Metodo della divisione}
\end{enumerate}

{}

$h(k) = k\,mod\,m$

{~~~~~~~~Esempio: }

$h=10,\,k=91,\,h(k)=91\,mod\,19=15$

{}

{Svantaggio: }

{Scegliere il valore di m diventa critico. }

{}

{Vantaggio:}

{Semplice da implementare}

{}

{~~~~~~~~Come scegliere m:}

{Si evitano le potenze di due per non ottenere sempre e solo i p bit
meno significativi ed evitare problemi (carattere uguale di fine
stringa). Scegliamo un numero primo non troppo vicino ad una potenza
esatta di 2 o 10.}

{Es: 3 collisioni accettabili, }\includegraphics{images/image257.png}

{}

{}

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  {Metodo della moltiplicazione}
\end{enumerate}

{~~~~~~~~Se $x \in [0\ldots1]$ uniformemente distribuiti}

$h(x) = \floor*{m*x}$

{~~~~~~~~Data una chiave naturale, la trasformo in un numero nell'intervallo $[0\ldots1]$ per poi moltiplicarlo per $m$.}

{~~~~~~~~Fisso una costante $A, 0 \leq A \leq 1$}

{~~~~~~~~Calcolo $K*A$ ed estraggo la parte frazionaria}

\begin{equation}
(K*A)\,mod\,1 = k * A - \floor*{k*A}
\end{equation}

\begin{equation}
h(x) = \floor*{m*(K*A)\,mod\,1}
\end{equation}

{Vantaggio:}

{Il valore di $m$ non è più critico. Funziona bene con tutti i valori di A. Knuth suggerì il valore $\frac{\sqrt{5}-1}{2}$.}


{~~~~~~~~Per semplificare i conti possiamo scegliere $m=2^p$}

{$A = \frac{q}{2w} = $ lunghezza di una parola in memoria, $0<q<2^w$ intero.}

{~~~~~~~~Ipotesi:}

{$k$ entra in una sola parola: $k*a = mod 1 = \frac{k*q}{2^w}\,mod\,1$}

{$h(k)=p$ bit più significativi della parola meno significativa di $k*q$.}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  {Hashing universale}
\end{enumerate}

{Se un avversario conosce la funzione hash, qualunque essa sia, potrebbe inserire nella tabella elementi che finiscono tutti nella stessa cella e ciò porterebbe a pessime prestazioni.}

{La soluzione è costruire un insieme di funzioni hash da pescare casualmente.}

\begin{enumerate}
\tightlist
\item
  {~Costruisco un insieme h di funzioni hash, l'insieme deve essere
  opportunamente costruito.}
\item
  {Il programma sceglie casualmente h dall' insieme}
\end{enumerate}

\subsection{Risoluzione delle collisioni tramite indirizzamento aperto}

{Ipotesi: }

{Non ho alcuna struttura ausiliaria esterna.}

{Idea:}

{Gli elementi sono tutti memorizzati nella tabella.}

{Non c'è memoria esterna}

{Ogni cella contiene un elemento dell'insieme dinamico oppure $NULL$}

{Per cercare un elemento di chiave $k$:}

\begin{enumerate}
\tightlist
\item
  {Calcoliamo $h$ ed esaminiamo la cella con indice (ispezione)}
\item
  {Se la cella contiene la chiave $k$, la ricerca ha successo. }
\item
  {Se invece contiene $NULL$, la ricerca termina senza successo.}
\item
  {Se la cella contiene una chiave che non è $k$ calcoliamo l'indice di un'altra cella in base a $k$ e all'ordine di ispezione. Si continua la scansione della tabella finché non si trova $k$ (successo), una cella contenente $NULL$ oppure dopo $m$ ispezioni senza successo.}
\end{enumerate}

{La funzione hash per l'indirizzamento aperto è la seguente:}

\begin{equation}
h:\mathbb{U}x\{0,0,\ldots,m-1\} \rightarrow \{0,0,\ldots,m-1\}
\end{equation}

{$h(k,i)$ rappresenta la posizione della chiave $k$ dopo $i$ ispezioni fallite}

{per ogni chiave, la sequenza di ispezioni data da \\
$<h(k,0),h(k,1),\ldots,h(k,m-1))>$ deve essere una permutazione di $<0,1,\ldots,m-1>$ \\ in modo che ogni posizione della tabella hash possa essere considerata come possibile cella in cui inserire una nuova chiave.}

{Assunzioni:}

{~~~~~~~~gli elementi della tabella hash sono senza dati satellite}

{Hash insert: restituisce l'indice della cella dove ha memorizzato la chiave $k$, oppure segnala un errore se la tabella è piena}

\lstinputlisting{code/hash_insert.txt}

{Hash search: restituisce $j$ se $T[j]$ contiene la chiave $k$ oppure $NULL$ se essa non esiste in $T$.}

\lstinputlisting{code/hash_search.txt}

{Problema:}

{La cancellazione da una tabella hash con indirizzamento aperto è problematica. }

{Soluzione:}

{Non si può cancellare ponendo $NULL$ al posto della chiave. Usiamo quindi un marcatore (un valore speciale, chiamato ``deleted'') al posto di $NULL$ per marcare una cella come vuota a causa di un'eliminazione.}

{Svantaggio:}

{Il tempo di ricerca non dipende più dal fattore di carico $\frac{m}{n}$}\\
{Non si usa l'indirizzamento aperto quando le chiavi vanno cancellate.
Si utilizza invece il concatenamento.}

{Hash\_Delete(Array T, Key
K)}\textsuperscript{\protect\hyperlink{cmnt18}{{[}r{]}}}

{La posizione viene determinata dalla funzione \\ $h:U\cup U\rightarrow \{0,1,\ldots,m-1\}$ che restituisce \\ $<h(k,0),h(k,1),\ldots,h(k,m-1)>$, permutazioni di $\{0,1,\ldots,m-1\}$. Le possibili permutazioni sono $m!$ ma ottenere un sufficiente numero di permutazioni distinte non è banale.}

{Estensione dell'hashing uniforme semplice: }

{Situazione ideale : }{$h$ deve rispettare la proprietà di }{hashing uniforme}{, ovvero ogni chiave deve avere la stessa probabilità di avere come sequenza di ispezione, una delle $n!$ permutazioni di $\{0,1,\ldots,m-1\}$}

{Per far ciò:}

{$h(k,0)$ deve distribuire in modo uniforme nelle $m$ celle.}

{$h(k,1)$ deve distribuire in modo uniforme nelle $m-1$ celle. (Nel caso la cella fosse occupata)}

{\ldots{}}

{$h(k,m-1)$ deve distribuire obbligatoriamente nell'unica cella ancora vuota.}

{Ovvero, $h$ deve rispettare la proprietà di Hashing Uniforme Semplice per ogni ispezione (o ``iterazione'') }

{Analizziamo quindi tre metodi di scansione}

\begin{enumerate}
\tightlist
\item
  {Ispezione (o ``scansione'') lineare}
\item
  {Ispezione (o ``scansione'') quadratica}
\item
  {Hashing doppio}
\end{enumerate}

\subsubsection{1. Ispezione (o ``scansione'') lineare}

{Data una funzione hash ordinaria $h':\mathbb{U} \rightarrow \{0,0,\ldots,m-1\}$ chiamata funzione ausiliaria, il metodo dell'ispezione lineare usa la seguente funzione:}

\begin{equation}
h(k,i) = (h'(k) + i)\,mod\,m
\end{equation}

{con $i \in \{0,0,\ldots,m-1\}$}

{Nota: la prima cella ispezionata determina l'intera sequenza diispezioni, quindi ci sono soltanto $m$ sequenze di ispezioni distinte.}

{Vantaggi: }

{Facilità di calcolo}

{Svantaggi}{: }

{Dopo $i$ celle occupate la proprietà che venga estratta la cella immediatamente successiva è $\frac{1+i}{m}$. Abbiamo quindi un problema di addensamento o aglomerazione primaria. Si possono formare lunghe file di celle occupate che aumentano il tempo di ricerca.}

{Per superare il limite delle $m$ ispezioni distinte e dell'addensamento, proviamo a cambiare il passo con una funzione quadratica.}

\subsubsection{2. Ispezione (o ``scansione'') quadratica}

{Utilizziamo una funzione di hashing quadratica.}

\begin{equation}
h(k,i) = (h'(k) + C_1*i + C_2 * i^2)\,mod\,m
\end{equation}

{con $C_1,C_2$ costanti non nulle ed $i \in \{0,1,\ldots,m-1\}$}

{La scansione quadratica funziona meglio}\textsuperscript{\protect\hyperlink{cmnt19}{{[}s{]}}}{~maparticolare attenzione va posta nella ricerca dei valori di $C_1,C_2$ in modo che vengano generati tutti gli indici. Non possono essere scegli in modo arbitrario.}

{Esempio: $C_1=C_2=\frac{1}{2}$,$m=2^p$}

{Ho un massimo di $m$ sequenze di ispezione distinte.}

{Svantaggio}{: ``Addensamento secondario''}

{Se due chiavi distinte $k_1 \neq k_2$ hanno valore hash ausiliario $h'(k_1) = h'(k_2)$ allora hanno la stessa sequenza di ispezione.}

{Nonostante tutto, continuo ad avere lo stesso passo tra un'iterazione e la successiva. Procedo quindo con una seconda funzione hash.}

\subsubsection{3. Hashing doppio}

\begin{equation}
h(k,i) = (h_1(k) + i*h_2(k))\,mod\,m
\end{equation}		

{~~~~~~~~Con $h_1,h_2$ funzioni hash ausiliarie e $i \in \{0,1,\ldots,m-1\}$}

{Vantaggio:}

{La posizione finale viene data dai valori combinati della coppia $(h_1(k),h_2(k))$, i cui elementi producono $m$ combinazioni distinte ciascuno.}

{Ho quindi $\Theta(m^2)$ sequenze di ispezione perchè ogni possibile coppia $(h_1(k),h_2(k))$ produce una sequenza distinta di ispezione.}

{Vogliamo porre $h_2(k)$ ed $m$ (dimensione della tabella hash) coprimi (relativamente primi). Ciò mi assicura che l'intera tabella venga ispezionata.}

\subsection{Esercizio}

{Date due ispezioni }\includegraphics{images/image300.png}{~con
}\includegraphics{images/image301.png}{coprimi, allora dimostrare che $i=i'$}

{Dimostrazione A}{:}

{Scelgo $m=2^p$ potenza di due e definisco $h_2(k)$ in modo che produca sempre un numero dispari: }\includegraphics{images/image304.png}

{Dimostrazione B}{:}

{Scelgo m primo, definisco $h_2(k)$ in modo che generi sempre un numero intero positivo minore di }\includegraphics{images/image235.png}{: }\includegraphics{images/image305.png}

{Esempio:}

$m=13$

$h_1(k)=k\,mod\,13$

$h_2(k)=1 + (k\,mod\,11)$

$h(k,i) = (h_1(k) + i * h_2(k))\,mod\,m$

{Input: $<79,50,69,72,98,14>$}

{}

$h(79,0)=1$, $h(50,0)=11$, $h(69,0)=4$, $h(72,0)=7$, 

$h(98,0)=7$

{~~~~~~~~Collisione: $h(98,1) = 5$}

$h(14,0)=5$

{Collisione: $h(14,1) = 9$}

\hypertarget{h.wkyvylqq2lrt}{\subsubsection{\texorpdfstring{{Analisi dell'hashing a indirizzamento aperto}}{Analisi dell'hashing a indirizzamento aperto}}\label{h.wkyvylqq2lrt}}

{}

\subsection{Teorema}

{Nell'ipotesi di}

\begin{itemize}
\tightlist
\item
  {Tabella hash priva di cancellazioni}
\item
  {Funzione hash che rispetta l'ipotesi di hashing uniforme}
\item
  {$\alpha = \frac{m}{n}$ fattore di carico. Essendo $n \leq m$, $0 \leq \alpha \leq 1$}
\end{itemize}

{il numero atteso (medio) di ispezioni in una ricerca senza successo è al massimo $\frac{1}{1-\alpha}$.}

\subsubsection{Dimostrazione}

{Essendo $\alpha \leq 1$ per ipotesi, sono presenti delle celle vuote.}

{La prima scansione avviene con probabilità 1.}

{La seconda scansione avviene con probabilità $\frac{m}{n} = \alpha$}

{La terza scansione avviene con probabilità $\frac{n}{m} * \frac{n-1}{m-1} \simeq \alpha^2 $}
\textsuperscript{\protect\hyperlink{cmnt20}{{[}t{]}}}

{Il valore atteso (medio) del numero di ispezioni sarà quindi:}

\begin{equation}
1+\alpha+\alpha^2+\ldots \leq \sum_{i=0}^{\infty}{\alpha^i} = \frac{1}{1-\alpha}
\end{equation}

{con $\alpha^i\leq1$}

{Se $\alpha$ è costante, una ricerca senza sucesso viene eseguita in tempo medio $\Theta(1)$ e in caso pessimo $O(n)$}

{Analisi del valore di $\alpha$:}

{Se $\alpha=0.5$ (tabella riempita a metà), allora il numero medio di ispezioni è $\frac{1}{1-0.5}=2$.}

{Se $\alpha=0.9$ (tabella riempita al 90\%),allora il numero medio di ispezioni è $\frac{1}{1-0.9}=10$.}

\subsection{Corollario}

{L'inserimento di un elemento in una tabella hash a indirizzamento aperto, con fattore di carico $\alpha$, richiede in media non più di $\frac{1}{1-\alpha}$ ispezioni (tempo richiesto da una ricerca senza successo) nell'ipotesi di Hashing Uniforme.}

{Nota: l'elemento viene inserito solamente se c'è almeno una cella vuota, quindi $\alpha < 1$}

{L'inserimento richiede una ricerca senza successo, seguita dalla sistemazione della chiave nella prima cella vuota. Quindi, dal teorema, il numero massimo di ispezioni sarà al massimo $\frac{1}{1-\alpha}$.}

\subsection{Teorema}

{Data una tabella hash ad indirizzamento aperto con $\alpha \leq 1$ e funzione hash che rispetti l'ipotesi di Hash Uniforme, in una ricerca con successo in una tabella le cui chiavi hanno uguale probabilità di essere scelte, il numero atteso di ispezioni è al massimo $\frac{1}{\alpha} * log(\frac{1}{1-\alpha})$.}

{Analisi del valore di $\alpha$:}

{Se $\alpha=0.5$ (tabella riempita a metà), allora il numero massimo di ispezioni è $1.387$, ovvero con meno di 2 accessi riesco a trovare l'elemento cercato.}

{Se $\alpha=0.9$ (tabella riempita al 90\%), allora il numero massimo di ispezioni è $2.255$, ovvero con meno di 3 accessi riesco a trovare l'elemento cercato.}

\subsection{Confrontro tra metodi di risoluzione delle collisioni}

{\includegraphics{images/image541.png}}

{Notiamo che i costi del concatenamento in nero (indirizzamento aperto) crescono molto più velocemente delle altre due rosse (liste di collisione).}

\section{Mod 2 - Grafi}

{Coppia ordinata $G=(V,E)$ formata da due insiemi $V$ (vertici) ed $E$ (archi)}

$V = \{1,2,3,\ldots,n\}$

{$E \subseteq VxV$, ovvero $E$ è un sottoinsieme dell'insieme delle parti (prodotto cartesiano) dell'insieme $V$}

{I grafi possono essere}

\begin{itemize}
\tightlist
\item
  {Orientati}
\item
  {Non orientati, se le relazioni sono simmetriche}
\end{itemize}

{~~~~~~~~se $\forall(u,v) \in E \iff (v,u) \in E$}

{Non si ammettono cappi nei grafi non orientati.}

$\forall u \in E \iff (u,u) \notin E$

\subsection{Sottografi}

{Sottografo di $G$}

{$G'=(V',E')$ è sottografo di $G$ se $V'\subseteq V$ e $E'\subseteq E$}

\paragraph{Sottografo  indotto}

{Dato $V' \subseteq V$, il sottografo indotto da $V'$ di $G$ è $G[V']=(V',E')$, con $E' = E\,\cap\,V\,x\,V'$}

\includegraphics{images/image534.png}

\subsection{Cammini}

{Un cammino di $G$ è una sequenza $<X_0,X_1,\ldots,X_k>$ dove i vertici appartengono al grafo di partenza.}

$\forall i,0\leq i\leq k,(X_i,X_{i+1})\in E$

\subsubsection{Cammini semplici e cammini non semplici}

$<1,2,3,4,5>$ è un cammino semplice\\
$<1,2,3,1,4,5>$ non è un cammino semplice

{Un cammino è semplice se tutti i vertici sono distinti}

\subsubsection{Lunghezza di un cammino}

{La lunghezza di un cammino è il numero dei suoi archi}

\subsubsection{Raggiungibilità dei vertici}

$u$ è raggiungibile da $v$ se esiste un cammino $<X_0,X_1,X_2,\ldots,X_k>$ tale che $X_0 = u$ e $X_k = v$

\subsubsection{Ciclo}

{Un ciclo è un cammino dove $X_0 = X_k$}

{Un grafo senza cicli si dice aciclico.}

\subsubsection{{[}NO{]}Grafo connesso}

{Un grado non orientato si dice connesso se $\forall (u,v) | u,v \in V$, esiste un cammino da $u$ a $v$ con $u\neq v$}

\subsubsection{{[}NO{]} Componente connessa}

{Dato $G=(V,E)$ e $V' \subseteq V$}

{$V'$ si dirà componente connessa se }

\begin{enumerate}
\tightlist
\item
  {$G[V']$ è connesso}
\item
  $V'${~non è sottoinsieme stretto di un sottografo connesso}
\end{enumerate}

{Se il numero di componenti connesse di un grafo è uguale a 1, }{il
grafo è connesso.}

\subsubsection{{[}NO{]}Vertici adiacenti}

{Dato $G=(V,E)$ e $u \in V$}

{Il grado di $u$, $deg(u)$ è il numero di vertici adiacenti a $u$}

\subsubsection{Arco incidente}

{Un arco che è collegato ad $v$}

\subsubsection{Vertici isolati e terminali}

{Se $deg(u) = 0$ allora $u$ è isolato}

{Se $deg(u) = 1$ allora $u$ è terminale}

\subsubsection{Teorema della stretta di mano (HandShaking Lemma)}

{{[}NO{]}}

\begin{equation}
\sum_{u \in V}{degree(u)} = 2m
\end{equation}

{dove $m = \abs{E}$ è il numero di archi.}

{{[}O{]}}

{$outDegree(u)$ : numero degli archi uscenti da $u$}

{$inDegree(u)$ : numero degli archi entranti in $u$}

\begin{equation}
\sum_{u \in V}{outDegree(u)} = \sum_{u \in V}{inDegree(u)} = m
\end{equation}

{{[}NO{]} Proprietà}

{Il numero di vertici che hanno grado dispari è sempre pari}

{Dim: }

\includegraphics{images/image366.png}

\includegraphics{images/image367.png}

\includegraphics{images/image368.png}

{}

\includegraphics{images/image369.png}

\includegraphics{images/image370.png}{~}

\includegraphics{images/image371.png}{~}

\includegraphics{images/image372.png}{~}

{}

\includegraphics{images/image373.png}

\includegraphics{images/image374.png}{~}

{Il numero di vertici con grado dispari è quindi pari}

{}

{Esercizio}{: }

{~~~~~~~~Dimostrare che, dato
}$G=(V,E)${~non orientato senza vertici
isolati (nessuno vertice ha grado 0),}

{~~~~~~~~con $\abs{E} = \abs{V} -1$}

{Allora, esistono almeno due vertici terminali}

{}

{Dimostrazione per assurdo:}

$n=\abs{V},m=\abs{E}$

{Per il lemma della stretta di mano:}

\includegraphics{images/image377.png}

\includegraphics{images/image378.png}

{Chiamo $V_1$ un sottoinsieme di $V$ di vertici terminali}

$V_1=\{u\in V\,|\,deg(u)=1\}$

{Il problema diventa: dimostrare che $\abs{V_1} \geq 2$}

\includegraphics{images/image382.png}

\includegraphics{images/image383.png}{????}\textsuperscript{\protect\hyperlink{cmnt21}{{[}u{]}}}

{$\abs{V_1} \geq 2$ OK}

\hypertarget{h.viqjw1dxzez8}{\subsection{\texorpdfstring{{Matrice di
adiacenza }}{Matrice di adiacenza }}\label{h.viqjw1dxzez8}}

{Dato }$G=(V,E)${~}

$n=\abs{V},m=\abs{E}$

{$A$ Matrice quadrata $n x n$ }

\begin{itemize}
\tightlist
\item
  {Per trovare il grado di un vertice basta calcolare la somma degli
  elementi sulla corrispondente riga}
\item
  {{[}NO{]}}$n${~archi in totale =
  doppia somma di tutti gli elementi della matrice}
\item
  {{[}O{]} }$n${~archi in totale =
  doppia somma di tutti gli elementi della matrice}
\end{itemize}

{}

{Usare le matrici di adiacenza solitamente conviene se ci sono molti
archi, altrimenti è meglio utilizzare le liste.}

\hypertarget{h.30h8k4akc7l}{\subsubsection{\texorpdfstring{{Matrice di adiacenza per grafi orientati}}{Matrice di adiacenza per grafi orientati}}\label{h.30h8k4akc7l}}


\begin{equation}
A_{i,j} = 
\begin{cases}
0 & \mbox{se } (i,j) \notin E \\ 
1 & \mbox{se } (i,j) \in E
\end{cases}
\end{equation}

\protect\hypertarget{t.baa371c993d5b7baad0f4dd74d8432f916b9230a}{}{}\protect\hypertarget{t.37}{}{}

{\includegraphics{images/image529.png}}

{}

{1}

{2}

{3}

{4}

{1}

{0}

{1}

{1}

{0}

{2}

{0}

{0}

{0}

{0}

{3}

{0}

{1}

{0}

{0}

{4}

{1}

{1}

{1}

{0}

{}

\hypertarget{h.iuwxcos21awu}{\subsubsection{\texorpdfstring{{Matrice di
adiacenza per grafi non
orientati}}{Matrice di adiacenza per grafi non orientati}}\label{h.iuwxcos21awu}}

\begin{equation}
A_{i,j} = 
\begin{cases}
0 & \mbox{se } \{i,j\} \notin E \\ 
1 & \mbox{se } \{i,j\} \in E
\end{cases}
\end{equation}


{Essendo la relazione tra vertici simmetrica,}\includegraphics{images/image390.png}

{}

{}

\protect\hypertarget{t.1db15af38187ce98dc87977567f42b70be1e9c99}{}{}\protect\hypertarget{t.38}{}{}

{\includegraphics{images/image535.png}}

{}

{1}

{2}

{3}

{1}

{0}

{1}

{1}

{2}

{1}

{0}

{0}

{3}

{1}

{0}

{0}

{}

{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{\texorpdfstring{{}}{}}\label{h.fq77zq901ude}

\hypertarget{h.qwq1tiruc9xg}{\subsection{\texorpdfstring{{Liste
concatenate}}{Liste concatenate}}\label{h.qwq1tiruc9xg}}

{}

{\includegraphics{images/image537.png}}

{}

{}

{Usare le liste di adiacenza solitamente conviene se ci sono pochi
archi, altrimenti è meglio utilizzare le matrici.}

{}

{{[}O{]} Densità}

\begin{equation}
\delta=\frac{numero\,di\,archi}{numero\,di\,possibili\,archi} = \frac{numero\,di\,archi}{n^2}
\end{equation}

$n=\abs{V},m=\abs{E}$

$\delta=\frac{m}{n^2}$

{{[}NO{]} Densità}

\begin{equation}
\delta=\frac{numero\,di\,archi}{numero\,di\,possibili\,archi}
\end{equation}

{$\delta=\frac{m}{k},\,k = \frac{n(n-1)}{2}$ coefficiente binomiale}

{}

{Grafi sparsi ($n\simeq m$): lista}

{Grafi densi ($n^2\simeq m$): matrice}

{}

{Proprietà:}

{Sia $G$ un grafo NO. Se $G$ è aciclico, allora la cardinalità $\abs{E} \leq \abs{V}-1$}

{Dimostrazione}{~induttiva su $n=\abs{V}$:}

{~~~~~~~~Caso base:}

{~~~~~~~~~~~~~~~~se $n=1$ allora $m = 0$

{~~~~~~~~~~~~~~~~se $n=2$ allora $m \leq 1$

{~~~~~~~~Passo induttivo $n \geq 3$:}



{{[}NO{]} Complemento di un grafo}

{Il complemento di un grafo è un grafo con gli stessi vertici, i cui
archi sono complementari:}

$G=(V,E), \overline{G}=(\overline{V},\overline{E})$

{}

\hypertarget{h.tsrwv4qy0dku}{\subsubsection{\texorpdfstring{{Grafo
autocomplementare}}{Grafo autocomplementare}}\label{h.tsrwv4qy0dku}}

{Un grafo è detto autocomplementare se $\overline{G}=G$ ovvero se
}\includegraphics{images/image405.png}

{\includegraphics{images/image532.png}}

{}

\hypertarget{h.akx1r9fzvwzo}{\subsection{\texorpdfstring{{Prodotto tra
matrici di
adiacenza}}{Prodotto tra matrici di adiacenza}}\label{h.akx1r9fzvwzo}}

{}

\hypertarget{h.c39ti8il3qtf}{\subsubsection{\texorpdfstring{{Prodotto di
matrici}}{Prodotto di matrici}}\label{h.c39ti8il3qtf}}

{Due matrici $n\,x\,m$ ed $p\,x\,n$ (il numero di righe della prima equivale al numero di colonne della seconda) possono essere moltiplicate tramite il prodotto ``righe per colonne''.}

$C = A*B$ \\
$C_{i,j}=\sum_{k=1}^{n}{a_{i,k}*a_{k,j}}$

\hypertarget{h.md5vuljp7xm}{\subsubsection{\texorpdfstring{{Matrice al
quadrato}}{Matrice al quadrato}}\label{h.md5vuljp7xm}}

{}

{Moltiplicando una matrice di adiacenza A per se stessa, tramite il
prodotto tra righe e colonne, ottengo una matrice delle stesse
dimensioni con le seguenti caratteristiche:}

\begin{itemize}
\tightlist
\item
  {Sulla diagonale principale ho i gradi dei vertici}
\item
  {Nelle altre posizioni ho il numero di cammini tra i e j di lunghezza
  2}
\end{itemize}

{}

$A^2 = A*A = (a^{(2)}_{i,j})$ \\
$a^{(2)}_{i,j}=\sum_{k=1}^{n}{a_{i,k}*a_{k,j}}$

{}

{Dimostrazione:}

{``Sulla diagonale principale ho i gradi dei vertici'' ($i=j$)}

$a^{(2)}_{i,j}=\sum_{k=1}^{n}{a_{i,k}*a_{k,i}}$

{Essendo $G$ non orientato, la matrice è simmetrica e $a_{i,k}*a_{k,i} = a^2_{i,k}$}

{$a^{(2)}_{i,i} = \sum^n_{k=1}{a^2_{i,k}}$ , notiamo che il termine della sommatoria contiene solo valori binari che elevati al quadrato restituiscono il medesimo valore. Perciò}

$a^{(2)}_{i,i} = \sum^n_{k=1}{a^2_{i,k}} = \sum^n_{k=1}{a_{i,k}} = deg(i)$

{``Nelle altre posizioni ho il numero di cammini tra $i$ e $j$ di lunghezza'' ($i\neq j$)}

{$a^2_{i,j}=\sum_{k=1}^{n}{a_{i,k}*a_{k,j}}$, essendo la matrice di adiacenza composta da valori binari, il prodotto risulta valere 1 solo se entrambi i fattori valgono 1, ovvero se esiste un cammino di lunghezza 2}

\subsubsection{Matrice con esponente maggiore di 2}

{$A^n, n > 2$ conterrà}

\begin{itemize}
\tightlist
\item
  {Sulla diagonale: il numero di cicli di lunghezza n che partono da i}
\item
  {Fuori dalla diagonale: il numero di cammini di lunghezza n}
\end{itemize}

{}

{Dimostrazione per
induzione:}\textsuperscript{\protect\hyperlink{cmnt22}{{[}v{]}}\protect\hyperlink{cmnt23}{{[}w{]}}}

{Ipotesi:}

{~~~~~~~~Passo induttivo:\\
\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}}\includegraphics{images/image418.png}

\includegraphics{images/image419.png}{, ovvero il
numero di cammini da }{i}{~a }{j}{~passanti per }{k}{.}

{\includegraphics{images/image538.png}}

\subsection{Esercizio}

{Mostrare che $G=(V,E) [NO]$ contiene un triangolo, ovvero un ciclo di lunghezza 3, se e solo se esistono due indici $i,j$ tali che sia la matrice di adiacenza $A$ e la matrice al quadrato $A^2$ hanno un elemento non nullo in posizione $(i,j)$}

\hypertarget{h.m8b6lpkoqj6x}{\subsection{\texorpdfstring{{{[}NO{]} Grafi regolari}}{{[}NO{]} Grafi regolari}}\label{h.m8b6lpkoqj6x}}

{$G=(V,E)$ si dice k-regolare se tutti i vertici hanno grado $k$. Con $n=\abs{V},m=\abs{E}$}

{\includegraphics{images/image524.png}}

\subsection{Proprietà}

{Se $G$ è 2-regolare allora il numero di vertici coincide necessariamente con il numero degli archi. $n=m$}

\subsubsection{Dimostrazione}

{Utilizziamo il teorema della stretta di mano}

$2m=\sum_{u\in V}{deg(u)}$\textsuperscript{\protect\hyperlink{cmnt24}{{[}x{]}}\protect\hyperlink{cmnt25}{{[}y{]}}}

\subsection{Esercizio}

{Dimostrare che se il grafo $G$ è 3-regolare, allora $n$ è pari. Analogamente, analizzare $G$ 4-regolare. }

\hypertarget{h.d5v408sc7po0}{\subsection{\texorpdfstring{{Isomorfismi di
grafi}}{Isomorfismi di grafi}}\label{h.d5v408sc7po0}}

{Un isomorfismo di grafi è un funzione che mappa un grafo in un secondo dalla stessa forma.}

{Viene fornita la definizione per i grafi non orientati, essa può però essere adattata a grafi orientati.}

\subsection{{[}NO{]} Definizione}

$G_1=(v_1,E_1),G_2(V_2,E_2)$

{$\Theta:V_1\rightarrow V_2$ si dice isomorfismo se valgono le due proprietà:}

\begin{enumerate}
\tightlist
\item
  {$\Theta$ deve essere una funzione biettiva (è necessaria una corrispondenza 1-1).}
\item
  {La relazione di adiacenza deve essere preservata\\
  $\forall u,v \in V_1, (u,v) \in E_1 \iff (\Phi(u),\Phi(v)) \in E_2$}
\end{enumerate}

{}

\hypertarget{h.umjijdswjd6a}{\subsubsection{\texorpdfstring{{Determinare
se due grafi sono isomorfi}}{Determinare se due grafi sono isomorfi}}\label{h.umjijdswjd6a}}

{}

{$G_1=(v_1,E_1),G_2(V_2,E_2)$ sono isomorfi se esiste un isomorfismo da $G_1$ a $G_2$ e si indica con $G_1\simeq G_2$}


{Condizioni necessarie perché $G_1\simeq G_2$:}

\begin{enumerate}
\tightlist
\item
  $\abs{V_1}=\abs{V_2}$
\item
  $\abs{E_1}=\abs{E_2}$
\item
  {Stessa degree sequence $degseq(G_1) = degseq(G_2)$}
\item
  $\#ComponentiConnesse(G_1) = 	\#ComponentiConnesse(G_2)$
\end{enumerate}

{}

{}

\protect\hypertarget{t.b6067c6d36f86489b9093689693d69077c2015d4}{}{}\protect\hypertarget{t.39}{}{}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
$G$\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
$\overline{G}$\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Può verificarsi?}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Connesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Connesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{FALSO}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Connesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Disconnesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{FALSO \\ (Es: autocomplementare)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Disconnesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Connesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{VERO}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Disconnesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Disconnesso}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{FALSO}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

{}

{Il problema di determinare l'esistenza di un'isomorfismo tra due grafi è esoso in termini di tempo. Per evitare il bruteforce di tutte le combinazioni si procede a mappare gli archi con grado analogo.}

\hypertarget{h.jnoiha3cadu8}{\section{\texorpdfstring{{Alberi}}{Alberi}}\label{h.jnoiha3cadu8}}

{Un albero è una particolare categoria di grafo}

\subsection{{[}NO{]}Definizione}

$G=(V,E)$ si dice albero {[}libero{]} se è aciclico e connesso.\\
Un albero libero in cui si seleziona un particolare vertice, chiamato ``radice'', si dice radicato.

{\includegraphics{images/image533.png}}

{Proprietà 1:}

Se $G$ è un albero, allora $\abs{E} = \abs{V} - 1$ poichè è, per definizione, connesso e aciclico:

\begin{itemize}
\tightlist
\item
  $G${~è connesso se $\abs{E} \geq \abs{V} - 1$ : si deve rispettare un numero minimo di archi}
\item
  $G${~è aciclico se $\abs{E} \leq \abs{V} - 1$ : si deve rispettare un numero massimo di archi}
\end{itemize}

{~~~~~~~~L'albero risulta essere quindi una struttura ``fragile'' per
quanto riguarda il numero di archi.}

{Proprietà degli alberi {[}liberi{]}}

{~~~~~~~~Sia }$G=(V,E) [NO]$ {, le seguenti affermazioni sono equivalenti:}

\begin{enumerate}
\tightlist
\item
  $G${~è un albero}
\item
  {Due vertici qualsiasi di }$G${~sono
  connessi da un unico cammino semplice, ovvero senza vertici ripetuti}
\end{enumerate}

{Dimostrazione}{:~~~~~~~~}

{Sia }$G=(V,E) [NO]$ {, per assurdo assumo ci siano almeno due vertici $u,v$, tali che esistono almeno due cammini che li collegano. Facendo ciò si forma un ciclo.}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  $G${~è connesso, ma se un qualche
  arco viene rimosso da }$G${, il
  grafo risultante è disconnesso}
\item
  {$G$ è connesso e $\abs{E}=\abs{V}-1$}
\item
  {$G$ è aciclico e $\abs{E}=\abs{V}-1$}
\item
  {$G$ è aciclico, ma se un qualunque arco viene aggiunto, allora il grafo risultante è ciclico.}
\end{enumerate}

{}

\hypertarget{h.9btor4ygb1hb}{\subsection{\texorpdfstring{{Alberi di
copertura}}{Alberi di copertura}}\label{h.9btor4ygb1hb}}

{Un albero $G=(V,E) [NO]$ connesso si dice di copertura se tutti gli archi in $T \subseteq E$ toccano tutti i vertici di $G$.}

\hypertarget{h.rdrr6bsx8ypo}{\subsubsection{\texorpdfstring{{Taglio di
un albero}}{Taglio di un albero}}\label{h.rdrr6bsx8ypo}}

{Un taglio di un albero è una divisione in due parti $(S,V\setminus S)$ dell'insieme dei vertici.}


{\includegraphics{images/image527.png}}


{Un taglio rispetta $A \subseteq E$ se non esistono archi di $A$ tagliati. Essi possono essere in una delle due partizioni ma non possono attraversare il taglio.}

\hypertarget{h.gfycwx7ubmbv}{\subsubsection{\texorpdfstring{{Arco
leggero}}{Arco leggero}}\label{h.gfycwx7ubmbv}}

{Un arco è leggero se ha il peso minore tra gli altri archi attraversanti un taglio.}

\hypertarget{h.1evqcdl7exzv}{\subsubsection{\texorpdfstring{{Albero di copertura minimo (MST)}}{Albero di copertura minimo (MST)}}\label{h.1evqcdl7exzv}}

{Sia $T$ un albero di copertura,definisco}

$w(T) = \sum_{(u,v)\in T}{w(u,v)}$

{dove $w:E\rightarrow \mathbb{R}$ è detta ``funzione peso''}

{Definizione:}

{Un albero di copertura }$T${~si dice minimo o ``di peso minimo'' (minimum spanning tree) se $w(T)$ è il minimo rispetto a tutti gli alberi di copertura}

\subsection{Teorema fondamentale degli MST}

{Sia $G=(V,E) [NO]$ connesso con funzione peso $w$.}

{Se le tre condizioni}

\begin{enumerate}
\tightlist
\item
  {$A \subseteq E$ è contenuto in qualche MST}
\item
  {$(S,V \setminus S)$ è un taglio che rispetta $A$}
\item
  {Sia $(u,v)$ un arco leggero che attraversa il taglio $(S,V \setminus S)$}
\end{enumerate}

{sono rispettate, allora l'arco $(u,v)$ è sicuro per $A$, ovvero $A \subseteq \{(u,v)\}$ è contenuto in qualche MST.}

\subsubsection{Dimostrazione}

{$A \subseteq T$ con $T$ MST}

{Due casi:}

\begin{enumerate}
\tightlist
\item
  {$(u,v) \in T$, caso banale in quanto $A \cup \{(u,v)\} \subseteq T$  e $T$ è l'MST che cerchiamo.}
\item
  {$(u,v) \notin T$, quindi $T$ non è l'MST che cerchiamo, procediamo quindi a costruirne uno:}
\end{enumerate}

{Per le sei proprietà fornite, se unisco $(u,v)$ a $T$ formo un ciclo, è quindi necessario rimuovere l'altro cammino $(x,y)$ che lo forma tra gli archi che attraversano il taglio. }

$T' = T \cup {(u,v)} \setminus \{(x,y)\} $

{\includegraphics{images/image530.png}}

$w(u,v) \leq w(x,y)${: Il peso dell'arco aggiunto è minore del peso dell'arco rimosso, in quanto esso è leggero per ipotesi.}

{Quanto vale allora $w(T')$? }

$w(T') \leq w(T)$ {ed essendo }$w(u,v) \leq w(x,y)${~allora $w(T') \leq w(T)$}

{Da }$w(T) \leq w(T') \leq w(T)$ \textsuperscript{\protect\hyperlink{cmnt26}{{[}z{]}}}{risulta che $w(T') = w(T)$, e }$T'${~è quindi un MST.}

{Nota}\textsuperscript{\protect\hyperlink{cmnt27}{{[}aa{]}}}{: se $w(u,v) = w(x,y)$ allora $(u,v) = (x,y)$?}

{Dobbiamo però dimostrare che $T'$ sia ``sicuro'', e perciò dimostriamo che \\ $A \cup \{(u,v)\} \subseteq T'$}

\subsection{Corollario}

{Sia $G=(V,E) [NO]$ connesso con funzione peso $w$. Se le tre condizioni}

\begin{enumerate}
\tightlist
\item
  {$A \subseteq E$ è contenuto in qualche MST}
\item
  {$C$ è una componente connessa della foresta $(V,A)$}
\item
  {Sia $(u,v)$ un arco leggero che collega la componente $C$ con il resto del grafo}
\end{enumerate}

{sono rispettate, allora l'arco $(u,v)$ è sicuro per $A$, ovvero $A \cup \{(u,v)\}$ è contenuto in qualche MST.}

\subsubsection{Dimostrazione}

{Se impongo che}

\begin{itemize}
\tightlist
\item
  {Il taglio dell'ipotesi 2 sia $(C,V\setminus C)$, in modo da isolare la componente connessa}
\item
  {MANCA}\textsuperscript{\protect\hyperlink{cmnt28}{{[}ab{]}}}
\end{itemize}

{posso utilizzare il teorema fondamentale per la dimostrazione}

\subsection{Corollario}

{Sia $(u,v)$ un arco di peso minimo in $G$. Allora $(u,v)$ appartiene a qualche MST.}

\subsubsection{Dimostrazione tramite la tecnica ``cuci e taglia''}

{Sia T un MST di G. }

{Due casi:}

\begin{enumerate}
\tightlist
\item
  $(u,v)\in T${, ovvio}
\item
  $(u,v)\notin T$
\end{enumerate}

\subsection{Corollario}

{Sia $(u,v)$ un arco di peso minimo in $G$ e supponiamo che sia unico. Allora $(u,v)$ appartiene a tutti gli MST}\textsuperscript{\protect\hyperlink{cmnt29}{{[}ac{]}}}{.}

\subsubsection{Dimostrazione per assurdo}

{Nonostante le ipotesi, esiste {[}almeno{]} un MST senza l'arco in
questione.}

{Non esistendo, provvediamo ad aggiungerlo a T. $T' = T \cup \{(u,v)\}$}

{Facendo ciò creo sicuramente un ciclo, in quanto $T$ è albero. Scelto un arco $(x,y)$ e lo rimuovo. Ho quindi costruito un albero di copertura con peso}\includegraphics{images/image470.png}

{Con $w(u,v) < w(x,y)$ (minore stretto!)}

{Contraddizione}{: essendo T MST, non può esistere un altro albero con
peso inferiore.}

\subsection{Esercizio}

$G=(V,E) [NO]$ connesso con $w:E \rightarrow \mathbb{R}$

{Sia $T_{min}$ un MST di G.}

{Sia $T'$ un albero di copertura (ST) non necessariamente minimo (M).}

{Siano $(u,v),(x,y)$ rispettivamente gli archi
di }\includegraphics{images/image475.png}{di peso massimo. Ordino gli
archi e considero i pesi maggiori.}

\includegraphics{images/image476.png}{~con}\includegraphics{images/image477.png}

\includegraphics{images/image478.png}{~con}\includegraphics{images/image479.png}

{Congettura: }$w(u,v) \leq w(x,y)${~}

{Si può procedere per confutazione con controesempio oppure per dimostrazione tramite metodo ``cuci e taglia''.}

{Soluzione}\textsuperscript{\protect\hyperlink{cmnt30}{{[}ad{]}}}{:}

\subsection{Esercizio}

{Dimostrare che, se tutti i pesi del grafo sono distinti, allora esiste un solo MST.}

{Soluzione}\textsuperscript{\protect\hyperlink{cmnt31}{{[}ae{]}}}{:}

\hypertarget{h.42rszsg7qe80}{\subsubsection{\texorpdfstring{{Generazione degli alberi di copertura minima}}{Generazione degli alberi di copertura minima}}\label{h.42rszsg7qe80}}

\protect\hypertarget{t.89c877068ffb6efc80378955af5aa00fb054212d}{}{}\protect\hypertarget{t.40}{}{}

\lstinputlisting{code/generic_mst.txt}

{Verranno analizzati gli algoritmi di Kruskal e Prim, i quali differiscono per l'implementazione della ricerca dell'arco sicuro.}

{Kruskall utilizza le strutture dati Set per la gestione di insiemi disgiunti. Esse dispongono di tre operatori:}

\begin{enumerate}
\tightlist
\item
  {Make\_set(X)}
\item
  {Union(x,y) o Merge\_set(x,y)}
\item
  {Find\_set(x)}
\end{enumerate}

{Esempio di utilizzo: determinare le componenti connesse di un grafo}

\lstinputlisting{code/componenti_connesse.txt}

\subsubsection{Generazione di MST : Kruskal}

$G=(V,E)$ connesso, $n=\abs{V},m=\abs{E}$

\lstinputlisting{code/kruskal.txt}

{*Per l'analisi delle due find\_set e della union ci viene fornita la complessità ottenuta tramite Heichmann.\\
}{Complessità}{:}

{$O(mlog(m)+n+mlog(m))$, avendo $m \geq n-1$ archi poichè l'arco è connesso, la complessità è $O(mlog(m))$}

\subsubsection{Simulazione di esecuzione}

\subsubsection{Tramite grafo}

{\includegraphics{images/image519.png}}

\subsubsection{Tramite tabella}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Passo}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{A}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{Insiemi disgiunti}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{1}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{2}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{(1,6)\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1,6\},\{2\},\{3\},\{4\},\{5\}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{3}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{(1,6),(1,4)\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1,4,6\},\{2\},\{3\},\{5\}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{4}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{(1,6),(1,4),(1,2)\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1,2,4,6\},\{3\},\{5\}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{5}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{(1,6),(1,4),(1,2),(4,3)\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1,2,3,4,6\},\{5\}}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{6}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{(1,6),(1,4),(1,2),(4,3),(4,5)\}}\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright\strut
{\{1,2,3,4,5,6\}}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{h.l5wypo8krmqc}{\subsubsection{\texorpdfstring{{Generazione di MST : Prim}}{Generazione di MST : Prim}}\label{h.l5wypo8krmqc}}

{A differenza di Kruskal, Prim richiede un vertice di partenza detto ``radice''. Con l'insieme Q si indica l'insieme dei vertici da estrarre.
$V\setminus Q$ indica quindi l'insieme dei vertici già estratti. $Q$ è una coda di priorità, implementata tramite Heap Binario, i cui elementi hanno le seguenti caratteristiche:}

\begin{itemize}
\tightlist
\item
  Predecessore: $\pi[u]$
\item
Chiave: $Key[u]$ valore dell'arco incidente che attraversa il taglio $(Q,Vsetminus Q)$ con peso minore. Si utilizza il valore infinito positivo per indicare l'eventuale assenza di archi.
\end{itemize}

\lstinputlisting{code/prim.txt}

La convergenza risulta essere finita

La complessità della funzione Prim è $O(m logn)$

{Simulazione}

{\includegraphics{images/image542.png}}

\subsection{Cammini minimi}

{Solitamente si sottointende l'utilizzo su grafi orientati, in caso di grafi non orientati, ogni arco può essere sostituito da una coppia di archi inversamente orientati.}

$G=(V,E,w)$ orientato, $w:E\rightarrow \mathbb{R}$

Un cammino $p$ è una sequenza $<x_0,x_1,\ldots,x_q>$, $\forall i, i\leq i \leq q, (x_{i-1},x_i) \in E$.

$w(p)=\sum_{i=1}^{q} w(x_{i-1},x_i)$ è la funzione ``peso'' di un cammino $p$.

{La distanza $\delta(u,v)$ tra due vertici $u,v \in V$ è definita nel seguente modo:}

{$\delta(u,v) = +\infty$ se non esiste un cammino orientato da u a v}

{$\delta(u,v)=min(w(p))$ minimo dei pesi dove $p$ è il cammino tra $u$ e $v$}

\subsubsection{Varianti}

{Esistono quattro diverse varianti di ricerva dei cammini minimi, combinazioni dei due casi si numerosità dei vertici di partenza e di destinazione.}

\begin{table}[]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{ll|l|l|}
\cline{3-4}
                                                 &                               & \multicolumn{2}{c|}{Destinazione}                            \\ \cline{3-4} 
                                                 &                               & \multicolumn{1}{c|}{Singola} & \multicolumn{1}{c|}{Multipla} \\ \hline
\multicolumn{1}{|c|}{}                           & \multicolumn{1}{c|}{Singola}  
& \specialcell{In: $G(V;E;w)\,u,v\in V$\\Out: $\delta(u,v)$ }                                                       
& \cellcolor[HTML]{F8A102}\specialcell{In: $G(V;E;w)\,s\in V$\\Out: $\forall v \in V, \delta(s,v)$}   \\ \cline{2-4} 
\multicolumn{1}{|c|}{\multirow{-2}{*}{Sorgente}} & \multicolumn{1}{c|}{Multipla} 
& \specialcell{In: $G(V;E;w)\,d\in V$\\Out: $\forall v \in V, \delta(v,d)$} 
& \cellcolor[HTML]{F8A102}\specialcell{In: $G(V;E;w)$\\Out: $\forall u,v \in V, \delta(u,v)$}    \\ \hline
\end{tabular}
\end{table}

{Solo i casi evidenziati verranno presi in analisi, in quanto gli altri due sono sottoproblemi di essi.}

\subsection{Archi con pesi negativi}}

{Ci domandiamo se sia possibile risolvere il problema degli archi con peso negativo sommando ai pesi di tutti gli archi del grafo $G$ la costante $k$ capace di renderli tutti positivi. Es: $k=-min(w(u,v))\,\forall u,v \in E$}

{NO. Sommare una costante ai pesi degli archi altera eventuali somme di pesi di archi di lunghezza differente in maniera diversa.}

\subsection{Strutture dati per la rappresentazione dei cammini minimi}

{Per ogni vertice $u \in V$ necessitiamo di due campi:}

\begin{enumerate}
\tightlist
\item
$d[u]$ : stima della distanza tra $s$ ed $u$
\item
$\pi[u]$ : predecessore
\end{enumerate}

\subsubsection{Sottografo dei predecessori}

{Dato $G=(V,E,w)$, il sottografo dei predecessori è $G_\pi=(V_\pi,E_\pi)$ dove}

$V_\pi=\{u \in V | \pi[u] \neq NIL\} \cup \{s\}$

$E_\pi=\{(\pi[u],u) \in E | u \in V_{\pi} \setminus \{s\}\}$

\subsubsection{Albero dei cammini minimi}

{Dato $G=(V,E,w)$, l'albero dei cammini minimi $G'=(V',E')$ è un sottografo di $G$ dove}

{$V'$ : tutti gli archi ragiungibili dal vertice sorgente}

{$G'$ : forma un albero radicato in $s$}

{$\forall v \in V'$ l'unico cammino tra $s$ e $v$ in $G'$ è un cammino minimo in $G$}

\subsubsection{Procedure di modifica dei due campi}

\lstinputlisting{code/init_single_source.txt}

\lstinputlisting{code/relax.txt}

\subsubsection{Dijkstra}

\lstinputlisting{code/dijkstra.txt}

{La coda di priorità $Q$ può essere implementata tramite}

\begin{enumerate}
\tightlist
\item
  {Array lineare}
\item
  {Heap binario}
\end{enumerate}

{Studiamo la complessità di Dijkstra con coda di priorità implementata con array lineare:}

\section{Correttezza di Dijkstra}

\subsection{Proprietà 1}

Un sottocammino $p'$ di un cammino minimo $p$, è anch'esso minimo.

\subsubsection{Dimostrazione}

Se, per assurdo, suppongo che $p'$ non sia minimo.\\
Allora deve esistere un terzo cammino $p''$ tra $x$ e $y$ con $w(p'') < w(p')$.\\
Se così fosse, $p$ non sarebbe minimo.

\subsection{Proprietà}

Dato $p$ cammino minimo, \\
$\delta(s,v)=\delta(s,u)+\delta(u,v)$ \\


\subsection{Proprietà 2 - Diseguaglianza triangolare}

$\delta(s,v)\leq \delta(s,u)+w(u,v)$ \\

\subsubsection{Dimostrazione}

caso a, $u$ non è raggiungibile da $s$, quindi $\delta(s,u) = +\infty$ \\
caso b, $u$ è raggiungibile da $s$, allora $\delta(s,u) < +\infty$ \\

se $p$ è minimo $\delta(s,v)=w(s,u)+w(u,v)$, altrimenti $\delta(s,v)<w(s,u)+w(u,v)$ \\


\subsection{Proprietà 3 - Limite inferiore}

\subsection{Proprietà 4 - Convergenza}

%\input{graphs/prop_4.tex}
%\digraph[scale=0.5]{abc}{rankdir=LR; a->b->c;}

{Dopo aver eseguito una Relax quindi avrò:}

$d[v] \leq d[u] + w(u,v)$

$d[v] = \delta(s,v) + w(u,v)$

$d[v] = \delta(s,v)$ 

Inoltre $\delta(s,v) \leq d[v]$ (Per il teorema del limite inferiore)

Perciò $d[v]=\delta(s,v)$

\subsection{Proprietà 5 - Sottografo dei predecessori ($G_\pi$)}

{In un qualsiasi algoritmo, se $\forall u \in V: d[u] = \delta(s,u)$, allora $G_\pi$ è un albero di cammini minimi}

\subsection{Teorema di Dijkstra}

{Dato $G=(V,E,w)$ orientato con $w:E\rightarrow \mathbb{R}$ non negativa ($w(u,v) \geq 0 \forall (u,v) \in E$) e $s$ sorgente. Allora alla fine dell'esecuzione dell'algoritmo di Dijkstra si avrà:}

\begin{itemize}
\tightlist
\item
{$\forall u \in V, d[u]=\delta(s,u)$ nel momento in cui il vertice $u$ viene estratto da $Q$.}
\item
{$G_\pi$ è albero di cammini minimi.}
\end{itemize}

\subsubsection{Dimostrazione}

{Dimostreremo che $\forall u \in V, d[u]=\delta(s,u)$ nel momento in cui il vertice $u$ viene estratto da $Q$.}

{Supponiamo per assurdo che $u\in V$, sia il primo vertice per cui, al momento della sua estrazione, $d[u] \neq \delta(s,u)$}

\paragraph{Osservazione 1:}

{Non può essere la sorgente poichè $\delta(s,s)=0=d[s]$, quindi $u\neq s$.}

\paragraph{Osservazione 2:}

$s \neq \emptyset$

\paragraph{Osservazione 3:}

Ci domandiamo se $u$ sia non raggiungibile da $s$, ovvero se $\delta(s,u)=+\infty$.
Una volta eseguita la Init\_SS disporrò di vertici con distanze $+\infty$.

$u$ è perciò raggiungibile da $s$.

\paragraph{Osservazione 4:}

\paragraph{Osservazione 5:}

$d[u] \leq d[y]$ (Poichè stà per essere estratto $y$ e non $u$)

\paragraph{Osservazione 6:}

\paragraph{Osservazione 7:}

$\delta(s,y) \leq \delta(s,u)$ (Poichè, per ipotesi, i pesi degli archi sono positivi)

\paragraph{Osservazione 8:}

$\delta(s,u) \leq d[u]$ (Per proprietà del limite inferiore)

\paragraph{Conclusioni:}

Perciò $d[u] \leq d[y] = \delta(s,y) \leq \delta(s,u) \leq d[u]$. \\ Quindi $d[u]= \delta(s,u)$ Assurdo, contraddizione.


\subsection{Algoritmo di Bellmann-Ford}

{L'algoritmo di Dijkstra risulta efficiente ma solo per grafi con archi i cui pesi sono tutti positivi. L'algoritmo di Bellmann-Ford ovvia a tale limitazione richiedendo però elevata potenza computazionale in quanto esso si basa sul brute-force. Rispetto a Dijkstra non sono più necessarie strutture dati e permette di segnalare eventuali cicli negativi.}

\protect\hyperlink{cmnt_ref1}{{[}a{]}}{Parte Intera Inferiore}

\protect\hyperlink{cmnt_ref2}{{[}b{]}}{?????}

\protect\hyperlink{cmnt_ref3}{{[}c{]}}{cammino più lungo verso una
foglia}

\protect\hyperlink{cmnt_ref4}{{[}d{]}}{Complessità O(log(n))}

\protect\hyperlink{cmnt_ref5}{{[}e{]}}{Parte Intera Inferiore}

\protect\hyperlink{cmnt_ref6}{{[}f{]}}{Complessità O(log(n))}

\protect\hyperlink{cmnt_ref7}{{[}g{]}}{parto da 1 (radice)}

\protect\hyperlink{cmnt_ref8}{{[}h{]}}{invariante}

\protect\hyperlink{cmnt_ref9}{{[}i{]}}{Complessità O(log(n))}

\protect\hyperlink{cmnt_ref10}{{[}j{]}}{Nell'implementazione reale ci si
comporta in maniera diversa, usando la parte finale della
HeapIncreaseKey}

\protect\hyperlink{cmnt_ref11}{{[}k{]}}{Complessità O(log(n))}

\protect\hyperlink{cmnt_ref12}{{[}l{]}}{Anomalia verso il basso}

{Complessità: O(log(n))}

\protect\hyperlink{cmnt_ref13}{{[}m{]}}{anomali verso il basso}

{Complessità : O(log(n))}

\protect\hyperlink{cmnt_ref14}{{[}n{]}}{Aggiunta mia}

\protect\hyperlink{cmnt_ref15}{{[}o{]}}{Che vuol dire???}

\protect\hyperlink{cmnt_ref16}{{[}p{]}}{per il countingSort}

\protect\hyperlink{cmnt_ref17}{{[}q{]}}{per i problemi della deleted}

\protect\hyperlink{cmnt_ref18}{{[}r{]}}{manca}

\protect\hyperlink{cmnt_ref19}{{[}s{]}}{???}

\protect\hyperlink{cmnt_ref20}{{[}t{]}}{Controllare}

\protect\hyperlink{cmnt_ref21}{{[}u{]}}{Capire la logica}

\protect\hyperlink{cmnt_ref22}{{[}v{]}}{INCOMPLETA}

\protect\hyperlink{cmnt_ref23}{{[}w{]}}{Pag 2- 14/3}

\protect\hyperlink{cmnt_ref24}{{[}x{]}}{MANCA ROBA}

\protect\hyperlink{cmnt_ref25}{{[}y{]}}{Pag 3 - 14/3}

\protect\hyperlink{cmnt_ref26}{{[}z{]}}{Perchè il primo termine??}

\protect\hyperlink{cmnt_ref27}{{[}aa{]}}{Eeeh\ldots{}}

\protect\hyperlink{cmnt_ref28}{{[}ab{]}}{MANCA}

\protect\hyperlink{cmnt_ref29}{{[}ac{]}}{Solitamente quando formulata
così, si procede per assurdo negando la tesi}

\protect\hyperlink{cmnt_ref30}{{[}ad{]}}{22/3/2018}

\protect\hyperlink{cmnt_ref31}{{[}ae{]}}{22/3/2018}

\end{document}
